{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Setup\n",
    "\n",
    "In this section I import all the necessary libraries and read the data from the dataset uploaded. I downloaded every csv files containing games from every pro leagues (LCS, LEC, LCK, EM...) from 2014 to 19/03/2025. \n",
    "For now, the model is training on theses files and I aim to update the data automatically every day. So the model will be able to retrain and refine his predictions based on the last trend.\n",
    "The data comes from [oracleselixir.com](https://oracleselixir.com/) which compile data from several different sources, including Match History pages, lolesports.com, lpl.QQ.com, Leaguepedia, the Riot Games solo queue APIs, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-11T03:47:23.242531Z",
     "iopub.status.busy": "2025-05-11T03:47:23.242188Z",
     "iopub.status.idle": "2025-05-11T03:47:27.886242Z",
     "shell.execute_reply": "2025-05-11T03:47:27.885080Z",
     "shell.execute_reply.started": "2025-05-11T03:47:23.242505Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder,LabelEncoder, FunctionTransformer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS\n",
    "from scipy.stats import pointbiserialr\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import glob\n",
    "import json\n",
    "import itertools\n",
    "pd.plotting.register_matplotlib_converters()\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from xgboost import XGBClassifier, plot_tree\n",
    "from joblib import cpu_count\n",
    "import os\n",
    "import gdown\n",
    "import sys\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "from supabase import Client, create_client\n",
    "\n",
    "print(\"Setup Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connection with database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T03:47:27.888489Z",
     "iopub.status.busy": "2025-05-11T03:47:27.888152Z",
     "iopub.status.idle": "2025-05-11T03:47:28.263366Z",
     "shell.execute_reply": "2025-05-11T03:47:28.262051Z",
     "shell.execute_reply.started": "2025-05-11T03:47:27.888462Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "url: str  = os.getenv(\"SUPABASE_URL\")\n",
    "service_role_key: str = os.getenv(\"SUPABASE_KEY\")\n",
    "supabase: Client = create_client(url, service_role_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Gathering Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T03:47:28.265677Z",
     "iopub.status.busy": "2025-05-11T03:47:28.265359Z",
     "iopub.status.idle": "2025-05-11T03:48:26.076327Z",
     "shell.execute_reply": "2025-05-11T03:48:26.075400Z",
     "shell.execute_reply.started": "2025-05-11T03:47:28.265650Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "working_folder = \"C:/Users/sacko/Documents/GitHub/LoL-predict/data/raw/\"\n",
    "url_share_raw_2025 = \"https://drive.google.com/file/d/1v6LRphp2kYciU4SXp0PCjEMuev1bDejc/view?usp=sharing\"\n",
    "url_dowload_raw_2025 = \"https://drive.google.com/uc?id=1v6LRphp2kYciU4SXp0PCjEMuev1bDejc\"\n",
    "output_path = \"C:/Users/sacko/Documents/GitHub/LoL-predict/data/raw/new_raw_data_actual_season.csv\"\n",
    "\n",
    "# Load every file of the data set\n",
    "if (os.path.exists(f\"./raw_data_all_years.csv\")):\n",
    "    df_raw_all_years = pd.read_csv(\"raw_data_all_years.csv\")\n",
    "else:  \n",
    "    folder_path = \"C:/Users/sacko/Documents/GitHub/LoL-predict/data/raw/\"\n",
    "    file_paths = glob.glob(folder_path + \"*.csv\")\n",
    "    df_raw_all_years = pd.concat([pd.read_csv(file, index_col=\"gameid\", low_memory=False, parse_dates=True) for file in file_paths])\n",
    "    df_raw_all_years.to_csv(\"../data/raw/raw_data_all_years.csv\")\n",
    "    print(\"df_raw_all_years\", df_raw_all_years.shape)\n",
    "    display(df_raw_all_years.head())\n",
    "\n",
    "\n",
    "### Loading File data\n",
    "gdown.download(url_dowload_raw_2025, output_path, quiet=False)\n",
    "X_2025 = pd.read_csv(\"C:/Users/sacko/Documents/GitHub/LoL-predict/data/raw/2025_LoL_esports_match_data_from_OraclesElixir.csv\", index_col=\"gameid\",low_memory=False, parse_dates=True)\n",
    "df_today_raw_season = pd.read_csv(f\"{output_path}\", index_col=\"gameid\", parse_dates=True, low_memory=False) # Raw data of this season downloaded today\n",
    "\n",
    "limited_paths = [\"C:/Users/sacko/Documents/GitHub/LoL-predict/data/raw/2020_LoL_esports_match_data_from_OraclesElixir.csv\",\n",
    "                         \"C:/Users/sacko/Documents/GitHub/LoL-predict/data/raw/2021_LoL_esports_match_data_from_OraclesElixir.csv\",\n",
    "                         \"C:/Users/sacko/Documents/GitHub/LoL-predict/data/raw/2022_LoL_esports_match_data_from_OraclesElixir.csv\",\n",
    "                         \"C:/Users/sacko/Documents/GitHub/LoL-predict/data/raw/2023_LoL_esports_match_data_from_OraclesElixir.csv\",\n",
    "    \"C:/Users/sacko/Documents/GitHub/LoL-predict/data/raw/2024_LoL_esports_match_data_from_OraclesElixir.csv\"\n",
    "]\n",
    "\n",
    "df_raw_previous_season = pd.concat([pd.read_csv(file, index_col=\"gameid\",  low_memory=False, parse_dates=True) for file in limited_paths])\n",
    "X_valid = pd.read_csv(limited_paths[-1], index_col=\"gameid\", low_memory=False, parse_dates=True)\n",
    "\n",
    "\n",
    "print(\"df_raw_previous_season Shape\", df_raw_previous_season.shape)\n",
    "display(df_raw_previous_season)\n",
    "\n",
    "print(\"X_valid Shape\", X_valid.shape)\n",
    "display(X_valid)\n",
    "\n",
    "print(\"X_2025 Shape\", X_2025.shape)\n",
    "display(X_2025.head())\n",
    "\n",
    "#print(X_2025.describe())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading local historic files and checking for new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T03:48:26.078398Z",
     "iopub.status.busy": "2025-05-11T03:48:26.078046Z",
     "iopub.status.idle": "2025-05-11T03:48:26.857453Z",
     "shell.execute_reply": "2025-05-11T03:48:26.856538Z",
     "shell.execute_reply.started": "2025-05-11T03:48:26.078370Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_saved_raw_season = pd.read_csv(f\"{working_folder}raw_data_actual_season.csv\") # Raw data of this season downloaded yesterday\n",
    "match_logs = pd.read_csv(f\"{working_folder}match_logs.csv\") # Key file storing prediction and true results\n",
    "X_saved_train = pd.read_csv(f\"{working_folder}X_train.csv\") # Training dataset of yesterday\n",
    "y_saved_train = X_saved_train[\"result\"]\n",
    "\n",
    "if (df_saved_raw_season.shape[0]== df_today_raw_season.shape[0]):\n",
    "    sys.exit(\"No new data detected today. Exit.\")\n",
    "print(f\"New data detected ! {df_today_raw_season.shape[0] - df_saved_raw_season.shape[0]} new rows\")\n",
    "print(f\"Numbers of rows yesterday : {df_saved_raw_season.shape[0]}\")\n",
    "print(f\"Numbers of rows today {pd.Timestamp.today().strftime('%Y-%m-%d')} : {df_today_raw_season.shape[0]}\")\n",
    "display(X_saved_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T03:48:26.859062Z",
     "iopub.status.busy": "2025-05-11T03:48:26.858618Z",
     "iopub.status.idle": "2025-05-11T03:48:26.863439Z",
     "shell.execute_reply": "2025-05-11T03:48:26.862293Z",
     "shell.execute_reply.started": "2025-05-11T03:48:26.859003Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Some constants\n",
    "cols_to_be_unique = [\"patch\",\"year\",\"playoffs\",\"split\",\"game\",\"date\",\"league\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Data Preprocessing\n",
    "In this section, we transform raw data into a format suitable for model training.\n",
    "\n",
    "### **1. Filtering Relevant Data**\n",
    "\n",
    "Certain columns are removed as they are not relevant to our analysis (e.g., `url`, `participantId`...).\n",
    "\n",
    "Additionally, we exclude incomplete games, as analyzing partially played matches would introduce bias into the results.\n",
    "\n",
    "### **2. Aggregating Data**\n",
    "\n",
    "The raw dataset contains individual player performances for each match. However, to simplify the model, we aggregate the data at the team level and do not consider individual performance.\n",
    "\n",
    "### **3. Preventing Data Leakage**\n",
    "\n",
    "Initially, each row in the dataset contains both the match result and the in-game statistics. Training a model on this information would cause **data leakage**, as we wouldn’t have access to match statistics before the game occurs.\n",
    "\n",
    "To prevent this, we replace in-game statistics with each team's historical performance before the match. After each game, these statistics are updated accordingly.\n",
    "\n",
    "### **Example:**\n",
    "\n",
    "- On **April 20, 2014**, FNC plays against SKT T1 with a current win rate of **75%**. The model trains on this data.\n",
    "- On **April 21, 2014**, FNC faces SSG. Their win rate is recalculated based on the previous match result, likely increasing slightly 😄. The model is then trained with the updated win rate.\n",
    "\n",
    "This approach ensures that predictions rely **only on information available before each match**, improving the model’s generalization and reliability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T03:48:26.865094Z",
     "iopub.status.busy": "2025-05-11T03:48:26.864664Z",
     "iopub.status.idle": "2025-05-11T03:48:26.983493Z",
     "shell.execute_reply": "2025-05-11T03:48:26.982259Z",
     "shell.execute_reply.started": "2025-05-11T03:48:26.865053Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#1. Filtering incomplete games and useless columns for the model winner series\n",
    "def winner_series_filtering(df):\n",
    "    # Drop games not complete\n",
    "    new_df = df[df[\"datacompleteness\"] == \"complete\"]\n",
    "\n",
    "    # Define columns to be removed and drop them (useless columns)\n",
    "    useless_columns = (\n",
    "        [\"participantid\", \"side\", \"position\", \"teamid\", \"teamkills\", \"teamdeaths\",\"url\"] +\n",
    "        [f\"ban{b}\" for b in range(1, 6)] +\n",
    "        [f\"pick{p}\" for p in range(1, 6)]\n",
    "    )\n",
    "    \n",
    "    new_df = new_df.drop(columns=useless_columns)\n",
    "        # Colonnes à garder même si elles ont une seule valeur\n",
    "    exceptions = set(cols_to_be_unique)\n",
    "    \n",
    "    # Colonnes à supprimer : elles ont une seule valeur et ne sont pas dans exceptions\n",
    "    cols_to_drop = [col for col in new_df.columns if new_df[col].nunique() <= 1 and col not in exceptions]\n",
    "    \n",
    "    # On les supprime\n",
    "    new_df = new_df.drop(columns=cols_to_drop)\n",
    "    \n",
    "    return new_df\n",
    "\n",
    "#X_2025 = winner_series_filtering(X_2025)\n",
    "\n",
    "#Select only game on EM to try\n",
    "X_2025_EM = X_2025[X_2025[\"league\"] == \"EM\"]\n",
    "X_2025_EM = winner_series_filtering(X_2025)\n",
    "\n",
    "# List of feature for my AI Intuition\n",
    "features = ['Team', 'TeamElo','WinRateLast10Game','WinStreaks','LoseStreaks','HeadToHead','AverageKills','AverageDeaths','AverageAssists','AverageDoubleKills','AverageTripleKills','AverageQuadraKills','AveragePentakills','AverageKpM','Patch','WinRateCurrentPatchA']\n",
    "features_engineered = ['TeamElo','WinRateLast10Game','WinStreaks','LoseStreaks','HeadToHead','Patch','WinRateCurrentPatchA']\n",
    "\n",
    "#print(X_2025_EM.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T03:48:26.984715Z",
     "iopub.status.busy": "2025-05-11T03:48:26.984413Z",
     "iopub.status.idle": "2025-05-11T03:48:27.033205Z",
     "shell.execute_reply": "2025-05-11T03:48:27.032181Z",
     "shell.execute_reply.started": "2025-05-11T03:48:26.984689Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "### 2. Select only the rows with the Team score and delete empty columns\n",
    "def select_teams_rows(df):\n",
    "    \n",
    "    new_df = df[df[\"playername\"].isnull()] # Select only team's row with aggregate data\n",
    "    print(\"Shape before selecting only teams rows\", new_df.shape)\n",
    "    series_empty = new_df.isna().sum()\n",
    "    columns_with_all_rows_empty = series_empty[series_empty == new_df.shape[0]].index.tolist()\n",
    "    columns_to_drop = [\"firstbloodkill\",\"firstbloodassist\",\"firstbloodvictim\",\"champion\"]\n",
    "    new_df = new_df.drop(columns=columns_with_all_rows_empty+ columns_to_drop, axis=1) # Drop full columns empty\n",
    "    new_df = new_df[(new_df.game.isnull() == False) & (new_df.teamname.isnull() == False)] \n",
    "    print(\"Shape after selecting only teams rows\", new_df.shape)\n",
    "    series_empty = new_df.isna().sum()\n",
    "    \n",
    "    #print(set(series_empty[series_empty != 0].index.tolist())-set(columns_mean)-set(columns_to_let_xgb)-set(columns_to_set_0))\n",
    "    \n",
    "    #print(\"\\n\".join(series_empty[series_empty != 0].index.astype(str)))\n",
    "    #print(len(series_empty[series_empty != 0].index))\n",
    "    #print(\"\\n\".join(f\"{index}: {value}\" for index, value in series_empty[series_empty != 0].items()))\n",
    "\n",
    "    #print(series_empty[series_empty != 0])\n",
    "    #new_df = new_df.dropna(axis=1)\n",
    "    return new_df\n",
    "    \n",
    "#X_2025_EM_TEAM_RESULTS = select_teams_rows(X_2025_EM)\n",
    "X_2025_EM = X_2025[X_2025[\"league\"] == \"EM\"]\n",
    "X_2025_EM = select_teams_rows(X_2025_EM)\n",
    "display(X_2025_EM)\n",
    "# display(test_limited[test_limited.champion.isnull() == False].iloc[0][\"champion\"])\n",
    "# display(X_2025_EM_TEAM_RESULTS.head())\n",
    "# print(X_2025_EM_TEAM_RESULTS.shape)\n",
    "#X_2025_EM_TEAM_RESULTS.to_csv('X_2025_EM_TEAM_RESULTS.csv')\n",
    "# drop year, label league + wr_league, keep split + wr_split, keep playoffs + wr_playoffs, keep patch + wr_patch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this point, we preprocess furthermore the data and aggregate from the team's row every BO.\n",
    "We will develop two different models:\n",
    "\n",
    "1. **winner_series** – This model predicts the outcome of a series (Best of X) without any prior knowledge except for the team names, the type of series (BO format), their historical statistics and of caurse the features we created.\n",
    "2. **score_series** – Based on the prediction from *winner_series*, this model forecasts the exact score of the series (e.g., 2-1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregating BO\n",
    "From here all the rows belonging to the same BO are compiled into 1 row.\n",
    "We drop the exact result (eg, 2-1) and take only the result of the series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T03:48:27.034718Z",
     "iopub.status.busy": "2025-05-11T03:48:27.034368Z",
     "iopub.status.idle": "2025-05-11T03:48:27.161364Z",
     "shell.execute_reply": "2025-05-11T03:48:27.160245Z",
     "shell.execute_reply.started": "2025-05-11T03:48:27.034676Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#3. Preventing Data Leakage, creating a new data with updated stats and feature engineered over time. \n",
    "\n",
    "\n",
    "def aggregate_bo(df, include_objects_columns=False):\n",
    "    \"\"\"\n",
    "    This function processes the given dataframe `df`, which contains match data, \n",
    "    by cleaning, grouping, and calculating statistics for each Best-of (Bo) series \n",
    "    and each team within the series.\n",
    "    \"\"\"\n",
    "\n",
    "    #assert not df.isnull().values.any(), \"Dataset contains missing values, cannot aggregate BO\"\n",
    "    \n",
    "    # Step 1: Sort the data by date and team name to organize the games\n",
    "    team_games = df.sort_values(by=[\"date\", \"gameid\",\"teamname\"]).copy()\n",
    "    counts_index = team_games.index.value_counts()\n",
    "    \n",
    "    #team_games = team_games[team_games.index.isin(counts_index[counts_index > 1].index)]\n",
    "    #display(team_games[team_games.index== \"ESPORTSTMNT02_2342149\"])\n",
    "    #display(team_games[team_games[\"date\"]==\"2025-01-23 16:22:48\"])\n",
    "    \n",
    "    # Step 2: Separate the team data for left and right teams based on row order\n",
    "    teams_stats_left = team_games.iloc[::2]  # Select rows for team A\n",
    "    teams_stats_right = team_games.iloc[1::2]  # Select rows for team B\n",
    "    #print(\"List of columns before creating A and B\",team_games.columns.tolist())\n",
    "    # Step 3: Join the stats of the two teams (team A and team B) into one DataFrame\n",
    "    teams_group = teams_stats_left.join(teams_stats_right, lsuffix=\"A\", rsuffix=\"B\")\n",
    "    # Make a unique column for same value between match \n",
    "    cols_b_drop = [col + 'B' for col in cols_to_be_unique]\n",
    "    teams_group = teams_group.drop(columns=cols_b_drop, axis=1) \n",
    "    teams_group.rename(columns={col+\"A\":col for col in cols_to_be_unique }, inplace=True) \n",
    "    \n",
    "    teams_group = teams_group[(teams_group[\"teamnameA\"]!=\"unknown team\") & (teams_group[\"teamnameB\"] !=\"unknown team\")]\n",
    "    teams_group = teams_group.dropna(subset=[\"teamnameA\",\"teamnameB\"],axis=0)\n",
    "    # Step 4: Initialize the dictionary to store unique Bo IDs and track the count\n",
    "    dict_bo = {\"id\": 0}\n",
    "    # Step 5: Define a function to identify the Bo series ID (`bo_id`) for each match\n",
    "    def identify_bo_id(row):\n",
    "        # Define a unique key for each Bo series based on the team names\n",
    "        if (isinstance(row.teamnameA, float) | isinstance(row.teamnameB, float)):\n",
    "            print(\"teamA\",row.teamnameA)\n",
    "            print(\"teamB\", row.teamnameB)\n",
    "            print(\"game\",row)\n",
    "            \n",
    "        key = row.teamnameA + \"_\" + row.teamnameB if row.teamnameA < row.teamnameB else row.teamnameB + \"_\" + row.teamnameA\n",
    "        \n",
    "        # Check if this is the first game of a Bo series\n",
    "        if row[\"game\"] == 1:  # If this is the first game of the Bo series\n",
    "            # Assign a new Bo ID for this series and increment the counter\n",
    "            dict_bo[key] = dict_bo[\"id\"] #row.name\n",
    "            dict_bo[\"id\"] += 1\n",
    "            return dict_bo[key]\n",
    "        else:\n",
    "            # If the Bo series already exists, return the existing Bo ID\n",
    "            return dict_bo.get(key)  # Return the Bo ID, if already exists\n",
    "    \n",
    "    # Step 6: Apply the `identify_bo_id` function to each row and add the Bo ID to the DataFrame\n",
    "    teams_group[\"bo_id\"] = teams_group.apply(identify_bo_id, axis=1)\n",
    "    # Step 7: Drop rows where `bo_id` is missing (if any) and convert `bo_id` to integer type\n",
    "    #teams_group.dropna(axis=0, inplace=True)\n",
    "    teams_group = teams_group[teams_group[\"bo_id\"].isnull() == False]\n",
    "    teams_group[\"bo_id\"] = teams_group[\"bo_id\"].astype(int)\n",
    "    # Step 8: Calculate the Bo type (Bo1, Bo3, Bo5) based on the number of games in the series\n",
    "    def get_type_bo(df):\n",
    "        nb_games = df.game.count()\n",
    "        if nb_games == 1:\n",
    "            return 1  # Bo1\n",
    "        elif nb_games == 2:\n",
    "            return 3  # Bo3\n",
    "        elif nb_games == 3:\n",
    "            # If Bo3, check if there was a winner by 3 games in a row\n",
    "            if df.resultA.mean() == 1 or df.resultB.mean() == 1:\n",
    "                return 5  # Bo5 won by 3 victories in a row\n",
    "            else:\n",
    "                return 3  # Bo3 with 1 defeat\n",
    "        else:\n",
    "            return 5  # Bo5 (4 or 5 games)\n",
    "\n",
    "    # Step 9: Calculate the mean statistics for each group (Bo series) while keeping 4 decimal precision\n",
    "    nums_cols = teams_group.select_dtypes(include=[\"number\"]).columns  # Get all numeric columns\n",
    "    X = teams_group.groupby(\"bo_id\")[nums_cols].mean().round(4)\n",
    "    \n",
    "    # Add object columns by taking the first value for each group\n",
    "    object_cols = teams_group.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "    if (\"bo_id\" in object_cols):\n",
    "        object_cols.remove(\"bo_id\")\n",
    "    # Use .transform to get the first value for each group for object columns\n",
    "    if (not include_objects_columns):\n",
    "        object_cols = [\"teamnameA\",\"teamnameB\"]\n",
    "\n",
    "    # Step 10: Add `teamnameA`, `teamnameB`, and `bo_type` for each group (Bo series)\n",
    "    X[[\"bo_type\"]+object_cols] = teams_group.groupby(\"bo_id\").apply(\n",
    "        lambda df: pd.Series({\n",
    "            \"bo_type\": get_type_bo(df),\n",
    "            **{col: df[col].iloc[0] for col in object_cols}\n",
    "        }),\n",
    "        include_groups=False\n",
    "    ).reset_index(drop=True)  # Reset the index to align the results correctly\n",
    "    \n",
    "    # Step 11: Clean up the resulting DataFrame by dropping unnecessary columns\n",
    "    X.drop(columns=[\"game\"], inplace=True)\n",
    "\n",
    "    # Return the final DataFrame with team statistics by Bo series\n",
    "    return X\n",
    "\n",
    "\n",
    "X_aggregated = aggregate_bo(X_2025_EM, True)\n",
    "display(X_aggregated)\n",
    "\n",
    "#create_team_stats_2(X_2025_EM_TEAM_RESULTS).to_csv(\"X_2025_EM_BO_RESULTS.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T03:48:27.165324Z",
     "iopub.status.busy": "2025-05-11T03:48:27.164982Z",
     "iopub.status.idle": "2025-05-11T03:48:27.190679Z",
     "shell.execute_reply": "2025-05-11T03:48:27.189500Z",
     "shell.execute_reply.started": "2025-05-11T03:48:27.165294Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def winner_series_create_features(df, include_objects_columns=False):\n",
    "    BASE_ELO = 1500  # Starting ELO for new teams\n",
    "    K_FACTOR = 45  # ELO adjustment speed\n",
    "    WINDOW_SIZE = 1  # Number of past matches to compute rolling averages\n",
    "    WINDOW_RECENT = 5\n",
    "\n",
    "    new_df = winner_series_filtering(df)\n",
    "    display(new_df)\n",
    "    new_df = select_teams_rows(new_df)\n",
    "    # Based on the code below commented, nums colomns that we want to make stats from the dataframe with team's row\n",
    "    nums_cols_avg_stats_fixed = ['gamelength', 'kills', 'deaths', 'assists', 'doublekills', 'triplekills', 'quadrakills', 'pentakills', 'firstblood', 'team kpm', 'ckpm', 'firstdragon', 'dragons', 'opp_dragons', 'elementaldrakes', 'opp_elementaldrakes', 'infernals', 'mountains', 'clouds', 'oceans', 'chemtechs', 'hextechs', 'elders', 'opp_elders', 'firstherald', 'heralds', 'opp_heralds', 'void_grubs', 'opp_void_grubs', 'firstbaron', 'barons', 'opp_barons', 'firsttower', 'towers', 'opp_towers', 'firstmidtower', 'firsttothreetowers', 'turretplates', 'opp_turretplates', 'inhibitors', 'opp_inhibitors', 'damagetochampions', 'dpm', 'damagetakenperminute', 'damagemitigatedperminute', 'wardsplaced', 'wpm', 'wardskilled', 'wcpm', 'controlwardsbought', 'visionscore', 'vspm', 'totalgold', 'earnedgold', 'earned gpm', 'goldspent', 'gspd', 'gpr', 'minionkills', 'monsterkills', 'cspm', 'goldat10', 'xpat10', 'csat10', 'opp_goldat10', 'opp_xpat10', 'opp_csat10', 'golddiffat10', 'xpdiffat10', 'csdiffat10', 'killsat10', 'assistsat10', 'deathsat10', 'opp_killsat10', 'opp_assistsat10', 'opp_deathsat10', 'goldat15', 'xpat15', 'csat15', 'opp_goldat15', 'opp_xpat15', 'opp_csat15', 'golddiffat15', 'xpdiffat15', 'csdiffat15', 'killsat15', 'assistsat15', 'deathsat15', 'opp_killsat15', 'opp_assistsat15', 'opp_deathsat15', 'goldat20', 'xpat20', 'csat20', 'opp_goldat20', 'opp_xpat20', 'opp_csat20', 'golddiffat20', 'xpdiffat20', 'csdiffat20', 'killsat20', 'assistsat20', 'deathsat20', 'opp_killsat20', 'opp_assistsat20', 'opp_deathsat20', 'goldat25', 'xpat25', 'csat25', 'opp_goldat25', 'opp_xpat25', 'opp_csat25', 'golddiffat25', 'xpdiffat25', 'csdiffat25', 'killsat25', 'assistsat25', 'deathsat25', 'opp_killsat25', 'opp_assistsat25', 'opp_deathsat25']\n",
    "    nums_cols_avg_stats = new_df.select_dtypes(include=\"number\").columns.tolist()\n",
    "    cols_to_remove = [\"game\",\"participantid\",\"teamkills\",\"teamdeaths\",\"result\"] + cols_to_be_unique\n",
    "    nums_cols_avg_stats = [x for x in nums_cols_avg_stats if x not in cols_to_remove]\n",
    "    league_avg = {stats: new_df[stats].mean() for stats in nums_cols_avg_stats} # Mean stats for the first game of team's\n",
    "    print(\"columns after select teams\",new_df.columns.tolist())\n",
    "    display(new_df)\n",
    "    dict_stats = {team: {col:[] for col in nums_cols_avg_stats } | {\"h2h\":[], \"result\":[], \"bo_1\":[],\"bo_3\":[],\"bo_5\":[], \"win_streak\":[], \"elo\":[BASE_ELO]} for team in set(new_df.teamname) }\n",
    "\n",
    "    # Initialize the first team with the base ELO \n",
    "    #1. S'assurer que la date est bien en format datetime\n",
    "    new_df['dateparsed'] = pd.to_datetime(new_df['date'])\n",
    "    \n",
    "    # 2. Calculer la limite de 6 mois depuis la date min\n",
    "    date_limite = new_df['dateparsed'].min() + pd.DateOffset(months=6)\n",
    "    \n",
    "    # 3. Identifier les équipes présentes pendant cette période\n",
    "    teams_initial_elo = new_df[new_df['dateparsed'] <= date_limite]['teamname'].unique()\n",
    "    \n",
    "    # 4. Initialiser leur ELO à 1500 dans dict_stats\n",
    "    for team in teams_initial_elo:\n",
    "        dict_stats[team]['elo'].append(BASE_ELO)\n",
    "    \n",
    "    new_df = aggregate_bo(new_df, True)    \n",
    "    new_df[\"result\"] = new_df.resultB.map(lambda row: 1 if row >0.5 else 0) # 0 if TeamA win , 1 if TeamB win\n",
    "    new_df = new_df.drop(columns=[\"resultA\",\"resultB\"])\n",
    "\n",
    "    # Constants\n",
    "    \n",
    "    \n",
    "    league_avg[\"elo\"] = BASE_ELO\n",
    "    \n",
    "    \n",
    "    #Function to calculate ELO updates\n",
    "    def calculate_elo(elo_A, elo_B, result_B, K):\n",
    "        expected_A = 1 / (1 + 10 ** ((elo_B - elo_A) / 400))\n",
    "        expected_B = 1 - expected_A\n",
    "        new_elo_A = elo_A + K * ((1-result_B) - expected_A)\n",
    "        new_elo_B = elo_B + K * (result_B - expected_B)\n",
    "        return new_elo_A, new_elo_B\n",
    "    \n",
    "    # Initialize empty DataFrame\n",
    "    # team_stats_df = pd.DataFrame(columns=[\"team\",\"elo\"] + nums_cols_avg_stats + [\"result\"])\n",
    "    \n",
    "    # # Function to get rolling averages safely\n",
    "    # def get_team_stat(team, col):\n",
    "    #     team_data = team_stats_df[team_stats_df[\"team\"] == team]\n",
    "    #     if len(team_data) >= WINDOW_SIZE:\n",
    "    #         return team_data[col].rolling(WINDOW_SIZE).mean().iloc[-1]\n",
    "    #     return league_avg[col]  # Default to league average if not enough data\n",
    "    \n",
    "    # Process each match\n",
    "    # Buffer pour stocker les valeurs calculées\n",
    "    data_buffer = []\n",
    "    \n",
    "    for i, row in new_df.iterrows():\n",
    "        team_A, team_B = row[\"teamnameA\"], row[\"teamnameB\"]\n",
    "        row_result = {}\n",
    "    \n",
    "        # ELO pré-match\n",
    "        # if (len(dict_stats[team_A][\"elo\"]) == 0):\n",
    "        #     elos = [stats[\"elo\"] for stats in dict_stats.values() if isinstance(stats, dict)]\n",
    "        #     mean_elo = np.percentile([e for sublist in elos for e in sublist], 75)\n",
    "        #     dict_stats[team_A][\"elo\"].append(mean_elo)\n",
    "        # if (len(dict_stats[team_B][\"elo\"]) == 0):\n",
    "        #     elos = [stats[\"elo\"] for stats in dict_stats.values() if isinstance(stats, dict)]\n",
    "        #     mean_elo = np.percentile([e for sublist in elos for e in sublist], 75)\n",
    "        #     dict_stats[team_B][\"elo\"].append(mean_elo)\n",
    "        pre_game_elo_A = dict_stats[team_A][\"elo\"][-1]\n",
    "        pre_game_elo_B = dict_stats[team_B][\"elo\"][-1]\n",
    "        row_result[\"elo_diff\"] = pre_game_elo_A - pre_game_elo_B\n",
    "    \n",
    "        # H2H win rate\n",
    "        h2h_key = team_A + team_B\n",
    "        h2h_win_rate_AvsB = np.mean(dict_stats[h2h_key]) if dict_stats.get(h2h_key) else 0.5\n",
    "        row_result[\"h2h_win_rate_AvsB\"] = h2h_win_rate_AvsB\n",
    "        dict_stats[team_A][\"h2h\"].append(h2h_win_rate_AvsB)\n",
    "        dict_stats[team_B][\"h2h\"].append(1 - h2h_win_rate_AvsB)\n",
    "\n",
    "        # Nb games diff\n",
    "        row_result[\"nb_games_diff\"] = len(dict_stats[team_A][\"result\"]) - len(dict_stats[team_B][\"result\"])\n",
    "\n",
    "        # new games\n",
    "        row_result[\"new_games\"] = (len(dict_stats[team_A][\"result\"]) <= 5 | len(dict_stats[team_B][\"result\"]) <=5)\n",
    "\n",
    "        # Win streaks\n",
    "        def compute_win_streak(results):\n",
    "            win_streak = 0\n",
    "            idx = len(results) - 1\n",
    "            while idx >= 0 and results[idx] == 1:\n",
    "                win_streak += 1\n",
    "                idx -= 1\n",
    "            return win_streak\n",
    "        \n",
    "        # Utilisation\n",
    "        win_streak_A = compute_win_streak(dict_stats[team_A][\"result\"])\n",
    "        win_streak_B = compute_win_streak(dict_stats[team_B][\"result\"])\n",
    "        dict_stats[team_A][\"win_streak\"].append(win_streak_A)\n",
    "        dict_stats[team_B][\"win_streak\"].append(win_streak_B)\n",
    "        # row_result[\"win_streakA\"] = win_streak_A\n",
    "        # row_result[\"win_streakB\"] = win_streak_B\n",
    "        row_result[\"win_streak_diff\"]  = win_streak_A - win_streak_B\n",
    "        \n",
    "        # Win rates globaux et par BO\n",
    "        team_A_res, team_B_res = dict_stats[team_A][\"result\"], dict_stats[team_B][\"result\"]\n",
    "        row_result[\"team_A_win_rate\"] = np.mean(team_A_res) if len(team_A_res) >= WINDOW_SIZE else 0.5\n",
    "        row_result[\"team_B_win_rate\"] = np.mean(team_B_res) if len(team_B_res) >= WINDOW_SIZE else 0.5\n",
    "\n",
    "        # Win rate BO Diff\n",
    "        win_rate_bo_A = np.mean(dict_stats[team_A][f\"bo_{row['bo_type']}\"]) if len(dict_stats[team_A][f\"bo_{row['bo_type']}\"]) >= WINDOW_SIZE else row_result[\"team_A_win_rate\"]\n",
    "        win_rate_bo_B = np.mean(dict_stats[team_B][f\"bo_{row['bo_type']}\"]) if len(dict_stats[team_B][f\"bo_{row['bo_type']}\"]) >= WINDOW_SIZE else row_result[\"team_B_win_rate\"]\n",
    "        row_result[\"win_rate_bo_diff\"] = win_rate_bo_A - win_rate_bo_B\n",
    "        for window in [3, 5, 10]:\n",
    "            win_rate_A = np.mean(team_A_res[-window:]) if len(team_A_res) >= window else row_result[\"team_A_win_rate\"]\n",
    "            win_rate_B = np.mean(team_B_res[-window:]) if len(team_B_res) >= window else row_result[\"team_B_win_rate\"]\n",
    "            row_result[f\"win_rate_last_{window}_diff\"] = win_rate_A - win_rate_B\n",
    "    \n",
    "        # Moyennes de stats\n",
    "        for col in nums_cols_avg_stats:\n",
    "            stat_A = np.mean(dict_stats[team_A][col]) if len(dict_stats[team_A][col]) >= WINDOW_SIZE else league_avg[col]\n",
    "            stat_B = np.mean(dict_stats[team_B][col]) if len(dict_stats[team_B][col]) >= WINDOW_SIZE else league_avg[col]\n",
    "            row_result[col + \"_diff\"] = stat_A - stat_B\n",
    "            dict_stats[team_A][col].append(row[col + \"A\"])\n",
    "            dict_stats[team_B][col].append(row[col + \"B\"])\n",
    "    \n",
    "        # Append dans le buffer\n",
    "        data_buffer.append(row_result)\n",
    "    \n",
    "        # Mise à jour ELO & résultats\n",
    "        new_elo_A, new_elo_B = calculate_elo(pre_game_elo_A, pre_game_elo_B, row[\"result\"], K_FACTOR)\n",
    "        dict_stats[team_A][\"elo\"].append(new_elo_A)\n",
    "        dict_stats[team_B][\"elo\"].append(new_elo_B)\n",
    "    \n",
    "        for key in [\"result\", f\"bo_{row['bo_type']}\"]:\n",
    "            dict_stats[team_A][key].append(1 - row[\"result\"])\n",
    "            dict_stats[team_B][key].append(row[\"result\"])\n",
    "    \n",
    "        dict_stats.setdefault(h2h_key, []).append(1 - row[\"result\"])\n",
    "    \n",
    "    # Fusion des colonnes calculées avec new_df\n",
    "    stats_df = pd.DataFrame(data_buffer)\n",
    "    new_df.reset_index(drop=True, inplace=True)\n",
    "    new_df = pd.concat([new_df, stats_df], axis=1)\n",
    "    new_df.index.name = \"bo_id\"\n",
    "    \n",
    "    # Calcul win_rate_diff final\n",
    "    new_df[\"win_rate_diff\"] = new_df[\"team_A_win_rate\"] - new_df[\"team_B_win_rate\"]\n",
    "    \n",
    "    return new_df, dict_stats\n",
    "\n",
    "#X_limited_EM = X_limited[(X_limited[\"league\"]==\"LFL\")  & (X_limited.split == \"Spring\")] # & (X_limited.date > \"2024-01-17 \") & (X_limited.date < \"2024-03-14 \")\n",
    "#X_intuition, dict_stats = winner_series_create_features(df_raw_previous_season, True)\n",
    "# X_intuition_clean = X_intuition.dropna(axis=1)\n",
    "#display(X_intuition)\n",
    "# Saving to csv\n",
    "# X_intuition.to_csv(\"X_intuition_2023-2025.csv\")\n",
    "# X_intuition_clean.to_csv(\"X_intuition_2023-2025_clean.csv\")\n",
    "# with open('dict_stats.json', 'w') as f:\n",
    "#     json.dump(dict_stats, f)\n",
    "# print(\"X_intuition dirty shape:\",X_intuition.shape)\n",
    "# print(\"X_intuition clean shape:\",X_intuition_clean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T03:48:27.193646Z",
     "iopub.status.busy": "2025-05-11T03:48:27.193319Z",
     "iopub.status.idle": "2025-05-11T03:48:27.210168Z",
     "shell.execute_reply": "2025-05-11T03:48:27.209133Z",
     "shell.execute_reply.started": "2025-05-11T03:48:27.193618Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# # ✅ 1. Assert no missing values (or handle them)\n",
    "# #assert not X_intuition.isnull().values.any(), \"Dataset contains missing values!\"\n",
    "\n",
    "# # # ✅ 3. Feature Engineering Function (Avoiding Data Leakage)\n",
    "# # def create_team_stats(df):\n",
    "# #     \"\"\"\n",
    "# #     Compute team-level statistics based ONLY on past matches (not future).\n",
    "# #     \"\"\"\n",
    "# #     df_sorted = df.sort_values(\"match_date\")  # Ensure chronological order\n",
    "# #     team_stats = df_sorted.groupby(\"team_id\").agg({\n",
    "# #         \"win\": \"mean\",   # Team win rate\n",
    "# #         \"gold_diff\": \"mean\",  # Average gold diff\n",
    "# #         \"kills\": \"mean\",  # Average kills\n",
    "# #         \"deaths\": \"mean\",  # Average deaths\n",
    "# #     }).reset_index()\n",
    "\n",
    "\n",
    "# # # Custom transformer to apply feature engineering ONLY on training data\n",
    "# # feature_engineering_transformer = FunctionTransformer(create_team_stats, validate=False)\n",
    "\n",
    "# # ✅ 4. Categorical Encoding (Label Encoding for teams & champions)\n",
    "\n",
    "# def only_diff(df):\n",
    "#     cols_df = df.columns.tolist()\n",
    "#     cols_A = [col for col in cols_df if col[-1]==\"A\"]\n",
    "#     cols_B = [col for col in cols_df if (col[-1]==\"B\") & (col !=\"h2h_win_rate_AvsB\")]\n",
    "#     assert len(cols_A) == len(cols_B), \"Taille de colonne A différente de colonne B\"\n",
    "#     df = df.drop(columns=(cols_A + cols_B))\n",
    "#     return df\n",
    "\n",
    "\n",
    "# # ✅ 2. Drop unnecessary columns\n",
    "# columns_to_drop = [\"teamnameA\",\"teamnameB\",\"league\",\"date\",'playoffs', 'patch']  # Example: Remove match ID and timestamp\n",
    "# # Columns with low MI & Correlation\n",
    "# col_low_MI = [] #wpm_diff better for LGB\n",
    "# # ✅ 5. Define Features and Target (Above at preprocessor)\n",
    "# # ✅ Define Features and Target\n",
    "# X = X_intuition_clean.drop(columns=([\"result\"] +columns_to_drop+ col_low_MI)) # Features\n",
    "# X = only_diff(X)\n",
    "# y = X_intuition[\"result\"]  # Target: 0 (Team A wins), 1 (Team A loses)\n",
    "\n",
    "# nums_cols = X.select_dtypes(include=\"number\").columns.tolist()\n",
    "# columns_to_set_0 = [\"quadrakills\",\"pentakills\",\"dragons\",\"opp_dragons\",\"elementaldrakes\",\"opp_elementaldrakes\",\"infernals\",\"mountains\",\"clouds\",\"oceans\",\"chemtechs\",\"hextechs\",\"dragons (type unknown)\",\"elders\",\"opp_elders\",\"firstherald\",\"heralds\",\"opp_heralds\",\"void_grubs\",\"opp_void_grubs\",\"firstbaron\",\"barons\",\"opp_barons\",\"firsttower\",\"towers\",'opp_towers', 'firstmidtower', 'firsttothreetowers', 'turretplates', 'opp_turretplates'] # Not here before or low impact\n",
    "# cat_cols = X.select_dtypes(exclude=\"number\").columns.tolist() \n",
    "# col_num_most_frequent = [\"firstblood\",\"firstdragon\"]\n",
    "# columns_mean = ['damageshare', 'earnedgoldshare', 'gspd', 'gpr', 'total cs', 'goldat10', 'xpat10', 'csat10', 'opp_goldat10', 'opp_xpat10', 'opp_csat10', 'golddiffat10', 'xpdiffat10', 'csdiffat10', 'killsat10', 'assistsat10', 'deathsat10', 'opp_killsat10', 'opp_assistsat10', 'opp_deathsat10', 'goldat15', 'xpat15', 'csat15', 'opp_goldat15', 'opp_xpat15', 'opp_csat15', 'golddiffat15', 'xpdiffat15', 'csdiffat15', 'killsat15', 'assistsat15', 'deathsat15', 'opp_killsat15', 'opp_assistsat15', 'opp_deathsat15', 'goldat20', 'xpat20', 'csat20', 'opp_goldat20', 'opp_xpat20', 'opp_csat20', 'golddiffat20', 'xpdiffat20', 'csdiffat20', 'killsat20', 'assistsat20', 'deathsat20', 'opp_killsat20', 'opp_assistsat20', 'opp_deathsat20', 'goldat25', 'xpat25', 'csat25', 'opp_goldat25', 'opp_xpat25', 'opp_csat25', 'golddiffat25', 'xpdiffat25', 'csdiffat25', 'killsat25', 'assistsat25', 'deathsat25', 'opp_killsat25', 'opp_assistsat25', 'opp_deathsat25']\n",
    "# col_most_important = ['team kpm_diff', 'gamelength_diff', 'bo_type', 'earnedgold_diff', 'wardskilled_diff', 'elo_diff', 'win_rate_diff']\n",
    "# col_best_perf = ['goldspent_diff', 'win_rate_last_3_diff', 'wardsplaced_diff', 'triplekills_diff', 'assists_diff', 'monsterkills_diff', 'firstblood_diff', 'elo_diff', 'win_rate_diff', 'h2h_win_rate_AvsB', 'wardskilled_diff', 'team kpm_diff', 'ckpm_diff', 'controlwardsbought_diff', 'totalgold_diff', 'damagetakenperminute_diff', 'gamelength_diff', 'wcpm_diff', 'bo_type']\n",
    "\n",
    "# # Create a custom transformer for LabelEncoding both columns consistently\n",
    "# def getColAandB(columns):\n",
    "#     colA = [col+\"A\" for col in columns]\n",
    "#     colB = [col+\"B\" for col in columns]\n",
    "#     return colA + colB\n",
    "\n",
    "# # Define the preprocessor pipeline with imputers for numeric and categorical columns\n",
    "# preprocessor = Pipeline(steps=[\n",
    "#     ('preprocessor', ColumnTransformer(\n",
    "#         transformers=[\n",
    "#             ('num_set_zero', SimpleImputer(strategy=\"constant\", fill_value=0), list(set(getColAandB(columns_to_set_0)) & set(nums_cols))),\n",
    "#             ('num_mean', SimpleImputer(strategy=\"mean\"), list(set(nums_cols)- set(getColAandB(columns_to_set_0))- set(getColAandB(col_num_most_frequent)) -set([\"patch\"]))),\n",
    "#             ('cat', Pipeline(steps=[\n",
    "#                 ('imputer', SimpleImputer(strategy=\"most_frequent\")),\n",
    "#                 ('label', OneHotEncoder())  # Apply consistent encoding to both teamnameA and teamnameB\n",
    "#             ]), cat_cols),\n",
    "#             ('num_most_frequent',SimpleImputer(strategy=\"most_frequent\"), list(set(getColAandB(col_num_most_frequent)+[\"patch\"]) & set(nums_cols)) )\n",
    "#         ]\n",
    "#     ))\n",
    "# ])\n",
    "\n",
    "# preprocessor_few_col = Pipeline(steps=[\n",
    "#     (\"preprocessor\", ColumnTransformer(\n",
    "#         transformers=[\n",
    "#             (\"num_col_mean\", SimpleImputer(strategy=\"mean\"), col_most_important)\n",
    "#         ],\n",
    "#         remainder='drop'\n",
    "#     ))\n",
    "# ])\n",
    "\n",
    "# preprocessor_best_perf = Pipeline(steps=[\n",
    "#     (\"preprocessor\", ColumnTransformer(\n",
    "#         transformers=[\n",
    "#             (\"num_col_mean\", SimpleImputer(strategy=\"mean\"), col_best_perf)\n",
    "#         ],\n",
    "#         remainder='drop'\n",
    "#     ))\n",
    "# ])\n",
    "\n",
    "# # ✅ 6. Train/Test Split (For Cross-validation)\n",
    "# X_clean = X.dropna(axis=1)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)\n",
    "# date_min = \"2023-01-10 17:07:15\"\n",
    "# date_split = X_valid.iloc[0,:].date\n",
    "# X_train_LFL = X_intuition_clean.drop(columns=([\"result\"] +columns_to_drop+col_low_MI))[(X_intuition_clean[\"date\"] >= date_min) & (X_intuition_clean[\"date\"] < date_split)]\n",
    "# X_train_LFL = only_diff(X_train_LFL)\n",
    "# y_train_LFL = X_intuition_clean[(X_intuition_clean[\"date\"] >= date_min) & (X_intuition_clean[\"date\"] < date_split)][\"result\"]\n",
    "# X_valid_LFL = X_intuition_clean.drop(columns=([\"result\"] +columns_to_drop+col_low_MI))[(X_intuition_clean[\"date\"] >= date_split)]\n",
    "# X_valid_LFL = only_diff(X_valid_LFL)\n",
    "# y_valid_LFL = X_intuition_clean[(X_intuition_clean[\"date\"] >= date_split)][\"result\"]\n",
    "# # ✅ 7. Pipeline: Feature Engineering + Encoding + Model\n",
    "# pipeline_random = Pipeline(steps=[\n",
    "#     #(\"feature_engineering\", feature_engineering_transformer),  # Compute team stats\n",
    "#     (\"preprocess\", preprocessor),  # Encode categorical features\n",
    "#     (\"model\", RandomForestClassifier(n_estimators=154, random_state=42, min_samples_split=10))\n",
    "# ])\n",
    "\n",
    "# model_lgb = lgb.LGBMClassifier(n_estimators=82, learning_rate=0.05, n_jobs=4, max_bin=101, verbose=-1)\n",
    "\n",
    "# pipeline_LGB_few_col = Pipeline([\n",
    "#     (\"preprocess\", preprocessor_few_col),\n",
    "#     (\"model\", model_lgb)\n",
    "# ])\n",
    "\n",
    "# pipeline_LGB_best_perf = Pipeline([\n",
    "#     (\"preprocess\", preprocessor_few_col),\n",
    "#     (\"model\", model_lgb)\n",
    "# ])\n",
    "# print(f\"Nb Columns {len(X.columns.tolist())}:\\n{X.columns.tolist()}\")\n",
    "\n",
    "# # pipeline_xgb = Pipeline([\n",
    "# #     (\"categorical_transformer\", categorical_transformer),\n",
    "# #     (\"mode\", XGBRegressor(n_estimators=1000, learning_rate=0.05, early_stopping_rounds=5))\n",
    "# # ])\n",
    "# # ✅ 8. Cross-Validation to Evaluate Model\n",
    "# model_xgb = XGBClassifier(n_estimators=154, learning_rate=0.05, n_jobs=4)\n",
    "# model_xgb_test = XGBClassifier(n_estimators=1500, learning_rate=0.05, early_stopping_rounds=100, n_jobs=4)\n",
    "# pipeline_LGB_best_perf.fit(X_train_LFL, y_train_LFL)\n",
    "# model_xgb_test.fit(X_train_LFL[['goldspent_diff', 'win_rate_last_3_diff', 'wardsplaced_diff', 'triplekills_diff', 'assists_diff', 'monsterkills_diff', 'firstblood_diff', 'elo_diff', 'win_rate_diff', 'h2h_win_rate_AvsB', 'wardskilled_diff', 'team kpm_diff', 'ckpm_diff', 'controlwardsbought_diff', 'totalgold_diff', 'damagetakenperminute_diff', 'gamelength_diff', 'wcpm_diff', 'bo_type']], y_train_LFL, eval_set=[(X_valid_LFL[['goldspent_diff', 'win_rate_last_3_diff', 'wardsplaced_diff', 'triplekills_diff', 'assists_diff', 'monsterkills_diff', 'firstblood_diff', 'elo_diff', 'win_rate_diff', 'h2h_win_rate_AvsB', 'wardskilled_diff', 'team kpm_diff', 'ckpm_diff', 'controlwardsbought_diff', 'totalgold_diff', 'damagetakenperminute_diff', 'gamelength_diff', 'wcpm_diff', 'bo_type']], y_valid_LFL)], verbose=False)\n",
    "# # y_pred_xgb = model_xgb.predict(X_test)\n",
    "# #skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "# # cv_scores = cross_val_score(pipeline_random, X_train_LFL, y_train_LFL, cv=5, scoring=\"accuracy\")\n",
    "# #cv_scores_xgb = cross_val_score(pipeline_XGB, X_train_LFL, y_train_LFL, cv=5, scoring=\"accuracy\")\n",
    "\n",
    "# # print(f\"Cross-validation Accuracy Random: {np.mean(cv_scores):.4f}\")\n",
    "# # print(f\"Cross-validation Accuracy XGB: {np.mean(cv_scores_xgb):.4f}\")\n",
    "# y_pred_xgb = model_xgb_test.predict(X_valid_LFL[['goldspent_diff', 'win_rate_last_3_diff', 'wardsplaced_diff', 'triplekills_diff', 'assists_diff', 'monsterkills_diff', 'firstblood_diff', 'elo_diff', 'win_rate_diff', 'h2h_win_rate_AvsB', 'wardskilled_diff', 'team kpm_diff', 'ckpm_diff', 'controlwardsbought_diff', 'totalgold_diff', 'damagetakenperminute_diff', 'gamelength_diff', 'wcpm_diff', 'bo_type']])\n",
    "# y_pred_lgb = pipeline_LGB_best_perf.predict(X_valid_LFL)\n",
    "# # print(\"\\n\\nXGB\\nBest iteration\", model_xgb_test.best_iteration)\n",
    "# print(f\"- Accuray X_test XGB: {accuracy_score(y_pred_xgb, y_valid_LFL):.4f}\",)\n",
    "# print(f\"- F1_Score X_test XGB: {f1_score(y_pred_xgb, y_valid_LFL):.4f}\")\n",
    "\n",
    "# # #print(\"\\n\\nXGB\\nBest iteration\", model_lgb.best_iteration)\n",
    "# print(f\"\\nLGB\\n- Accuray X_test LGB: {accuracy_score(y_pred_lgb, y_valid_LFL):.4f}\")\n",
    "# print(f\"- F1_Score X_test LGB: {f1_score(y_pred_lgb, y_valid_LFL):.4f}\")\n",
    "\n",
    "\n",
    "# # ✅ 9. Hyperparameter Tuning with Grid Search\n",
    "# param_grid = {\n",
    "#     \"model__n_estimators\": [50, 100, 200],\n",
    "#     \"model__max_depth\": [None, 10, 20],\n",
    "#     \"model__min_samples_split\": [2, 5, 10]\n",
    "# }\n",
    "\n",
    "# # grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring=\"accuracy\", n_jobs=-1)\n",
    "# # grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # # ✅ 10. Best Model & Evaluation\n",
    "# # best_model = grid_search.best_estimator_\n",
    "# # print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "\n",
    "# # # ✅ 11. Final Evaluation on Test Set\n",
    "# # test_accuracy = best_model.score(X_test, y_test)\n",
    "# # print(f\"Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Achievements\n",
    "\n",
    "model_xgb_test = XGBClassifier(n_estimators=1500, learning_rate=0.05, early_stopping_rounds=100, n_jobs=4,enable_categorical=True)\n",
    "- Accuracy of 0.66%\n",
    "- F1_Score of 0.65%\n",
    "\n",
    "### Without wpm_diff\n",
    "XGB\n",
    "Best iteration 141\n",
    "- Accuray X_test XGB: 0.6634\n",
    "- F1_Score X_test XGB: 0.6541\n",
    "\n",
    "LGB\n",
    "- Accuray X_test LGB: 0.6636\n",
    "- F1_Score X_test LGB: 0.6517\n",
    "\n",
    "### 101 max_bin, last_X_diff\n",
    "\n",
    "XGB\n",
    "Best iteration 88\n",
    "- Accuray X_test XGB: 0.6611\n",
    "- F1_Score X_test XGB: 0.6499\n",
    "\n",
    "LGB\n",
    "Best iteration 96\n",
    "- Accuray X_test LGB: 0.6653\n",
    "- F1_Score X_test LGB: 0.6564\n",
    "\n",
    "\n",
    "## Comparaison Brute Force 2023 entraînement prediction 2024\n",
    "Nb Columns 34:\n",
    "['year', 'bo_type', 'elo_diff', 'h2h_win_rate_AvsB', 'team_A_win_rate', 'team_B_win_rate', 'win_rate_bo_diff', 'win_rate_last_3_diff', 'win_rate_last_5_diff', 'win_rate_last_10_diff', 'gamelength_diff', 'kills_diff', 'deaths_diff', 'assists_diff', 'doublekills_diff', 'triplekills_diff', 'firstblood_diff', 'team kpm_diff', 'ckpm_diff', 'damagetochampions_diff', 'dpm_diff', 'damagetakenperminute_diff', 'damagemitigatedperminute_diff', 'wardsplaced_diff', 'wpm_diff', 'wardskilled_diff', 'wcpm_diff', 'controlwardsbought_diff', 'totalgold_diff', 'earnedgold_diff', 'earned gpm_diff', 'goldspent_diff', 'monsterkills_diff', 'win_rate_diff']\n",
    "XGB\n",
    "Best iteration 54\n",
    "- Accuray X_test XGB: 0.6496\n",
    "- F1_Score X_test XGB: 0.6270\n",
    "\n",
    "LGB\n",
    "Best iteration 53\n",
    "- Accuray X_test LGB: 0.6559\n",
    "- F1_Score X_test LGB: 0.6389\n",
    "\n",
    "## Brute force result\n",
    "- New best_perf 0.6649 with ['elo_diff', 'gamelength_diff', 'wardskilled_diff', 'win_rate_diff']\n",
    "- New best_perf 0.6684 with ['h2h_win_rate_AvsB', 'controlwardsbought_diff', 'monsterkills_diff', 'team kpm_diff', 'wcpm_diff', 'elo_diff', 'win_rate_diff']. \n",
    "- New best_perf 0.6688 with ['team kpm_diff', 'gamelength_diff', 'bo_type', 'earnedgold_diff', 'wardskilled_diff', 'elo_diff', 'win_rate_diff'].\n",
    "- New best_perf 0.6709 with ['goldspent_diff', 'win_rate_last_3_diff', 'wardsplaced_diff', 'triplekills_diff', 'assists_diff', 'monsterkills_diff', 'firstblood_diff', 'elo_diff', 'win_rate_diff', 'h2h_win_rate_AvsB', 'wardskilled_diff', 'team kpm_diff', 'ckpm_diff', 'controlwardsbought_diff', 'totalgold_diff', 'damagetakenperminute_diff', 'gamelength_diff', 'wcpm_diff', 'bo_type'].\n",
    "- New best_perf 0.6677 with ['playoffs', 'csdiffat15_diff', 'firstbaron_diff', 'goldspent_diff', 'win_rate_last_3_diff', 'wardsplaced_diff', 'triplekills_diff', 'assists_diff', 'monsterkills_diff', 'firstblood_diff', 'elo_diff', 'win_rate_diff', 'h2h_win_rate_AvsB', 'wardskilled_diff', 'team kpm_diff', 'ckpm_diff', 'controlwardsbought_diff', 'totalgold_diff', 'damagetakenperminute_diff', 'gamelength_diff', 'wcpm_diff', 'bo_type', 'win_rate_bo_diff', 'nb_games_diff', 'win_streak_diff']. \n",
    "- New best_perf 0.6703 with ['csdiffat15_diff', 'firstbaron_diff', 'goldat10_diff', 'goldspent_diff', 'win_rate_last_3_diff', 'wardsplaced_diff', 'triplekills_diff', 'assists_diff', 'monsterkills_diff', 'firstblood_diff', 'elo_diff', 'win_rate_diff', 'h2h_win_rate_AvsB', 'wardskilled_diff', 'team kpm_diff', 'ckpm_diff', 'controlwardsbought_diff', 'totalgold_diff', 'damagetakenperminute_diff', 'gamelength_diff', 'wcpm_diff', 'bo_type', 'win_rate_bo_diff', 'nb_games_diff', 'win_streak_diff']. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mutual Information & Correlation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brute force of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T03:48:27.211571Z",
     "iopub.status.busy": "2025-05-11T03:48:27.211209Z",
     "iopub.status.idle": "2025-05-11T03:48:27.229142Z",
     "shell.execute_reply": "2025-05-11T03:48:27.228128Z",
     "shell.execute_reply.started": "2025-05-11T03:48:27.211523Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "enable_efs = False\n",
    "if (enable_efs):\n",
    "    efs1 = EFS(best_model_lgb, \n",
    "               min_features=8,\n",
    "               max_features=9,\n",
    "               scoring='accuracy',\n",
    "               print_progress=True,\n",
    "               cv=3)\n",
    "    \n",
    "    efs1 = efs1.fit(X_valid_LFL, y_valid_LFL)\n",
    "    \n",
    "    print('Best accuracy score: %.2f' % efs1.best_score_)\n",
    "    print('Best subset (indices):', efs1.best_idx_)\n",
    "    print('Best subset (corresponding names):', efs1.best_feature_names_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T03:48:27.230544Z",
     "iopub.status.busy": "2025-05-11T03:48:27.230193Z",
     "iopub.status.idle": "2025-05-11T03:48:27.243400Z",
     "shell.execute_reply": "2025-05-11T03:48:27.242089Z",
     "shell.execute_reply.started": "2025-05-11T03:48:27.230508Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Test Prdedict on LFL 2024 \n",
    "# X_intuition_LFL, dict_stats_2 = winner_series_create_features(X_valid[(X_valid[\"league\"] == \"LFL\") & (X_valid[\"date\"] < \"2024-03-16\")], True)\n",
    "# display(X_intuition_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model expert on 1 league"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T03:48:27.245065Z",
     "iopub.status.busy": "2025-05-11T03:48:27.244631Z",
     "iopub.status.idle": "2025-05-11T03:48:27.258603Z",
     "shell.execute_reply": "2025-05-11T03:48:27.257657Z",
     "shell.execute_reply.started": "2025-05-11T03:48:27.245001Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# leagues = X_limited.league.unique()\n",
    "# print(leagues)\n",
    "# y_all = X_intuition_all.result\n",
    "# X_all = X_intuition_all[X_train.columns.tolist()]\n",
    "# print(X_all.columns.tolist())\n",
    "# X_all = X_all[X_intuition_all.league !=\"EM\"]\n",
    "# #X_all = X_all.drop(columns=[\"result\",\"teamnameA\",\"teamnameB\",\"leagueA\",\"leagueB\",\"dateA\",\"dateB\"])\n",
    "# y_pred_all = model_xgb_test.predict(X_all)\n",
    "# print(\"Accuray X_test XGB all: \",accuracy_score(y_pred_all, y_all[X_intuition_all.leagueA !=\"EM\"]))\n",
    "# for league in leagues:\n",
    "#     if (X_all[X_intuition_all.leagueA == league].shape[0] > 0):\n",
    "#         y_pred_league = model_xgb_test.predict(X_all[X_intuition_all.leagueA == league])\n",
    "#         print(f\"Accuray X_test XGB {league}: \",accuracy_score(y_pred_league, y_all[X_intuition_all.leagueA ==league]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphs evolution of a team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T03:48:27.260137Z",
     "iopub.status.busy": "2025-05-11T03:48:27.259733Z",
     "iopub.status.idle": "2025-05-11T03:48:27.276136Z",
     "shell.execute_reply": "2025-05-11T03:48:27.274921Z",
     "shell.execute_reply.started": "2025-05-11T03:48:27.260100Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# def transform_team_data(team_data):\n",
    "#     all_rows = []\n",
    "\n",
    "#     for team_name, stats in team_data.items():\n",
    "#         if not isinstance(stats, dict):\n",
    "#             #print(f\"⛔ Équipe ignorée (structure invalide) : {team_name} → {type(stats).__name__}\")\n",
    "#             continue  # Ignore les équipes mal formatées\n",
    "\n",
    "#         # On sécurise les longueurs\n",
    "#         list_lengths = [len(v) for v in stats.values() if isinstance(v, list)]\n",
    "#         if not list_lengths:\n",
    "#             print(f\"⚠️ Aucune stat de type liste pour l’équipe : {team_name}\")\n",
    "#             continue\n",
    "\n",
    "#         num_matches = len(next(iter(stats.values())))  # nombre de matchs = longueur d'une des listes\n",
    "#         for match_idx in range(num_matches):\n",
    "#             row = {'team': team_name, 'match_id': match_idx}\n",
    "#             for stat_name, values in stats.items():\n",
    "#                 row[stat_name] = values[match_idx]\n",
    "#             all_rows.append(row)\n",
    "#     df = pd.DataFrame(all_rows)\n",
    "#     return df\n",
    "# df_graph = transform_team_data(dict_stats_2)\n",
    "# print(df_graph.columns.tolist())\n",
    "# print(X_intuition_LFL.date.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T03:48:27.277681Z",
     "iopub.status.busy": "2025-05-11T03:48:27.277293Z",
     "iopub.status.idle": "2025-05-11T03:48:27.295834Z",
     "shell.execute_reply": "2025-05-11T03:48:27.294649Z",
     "shell.execute_reply.started": "2025-05-11T03:48:27.277638Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10,6))\n",
    "# for team in df_graph[\"team\"].unique():\n",
    "#     team_df = df_graph[df_graph[\"team\"]==team]\n",
    "#     plt.plot(team_df[\"match_id\"], team_df[\"elo\"], label=team)\n",
    "\n",
    "# plt.xlabel(\"Match ID\")\n",
    "# plt.ylabel(\"ELO\")\n",
    "# plt.title(\"ELO par Équipe et Match ID\")\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# # Création d'une nouvelle colonne avec les résultats cumulés\n",
    "# df_graph['result_cumule'] = df_graph.groupby('team')['result'].cumsum()\n",
    "\n",
    "# plt.figure(figsize=(10,6))\n",
    "# for team in df_graph[\"team\"].unique():\n",
    "#     team_df = df_graph[df_graph[\"team\"]==team]\n",
    "#     plt.plot(team_df[\"match_id\"], team_df[\"result_cumule\"], label=team)\n",
    "\n",
    "# plt.xlabel(\"Match ID\")\n",
    "# plt.ylabel(\"Result\")\n",
    "# plt.title(\"Result par Équipe et Match ID\")\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "# plt.figure(figsize=(10,6))\n",
    "\n",
    "# for team in df_graph['team'].unique():\n",
    "#     team_df = df_graph[df_graph['team'] == team]\n",
    "#     plt.scatter(team_df['result_cumule'], team_df['elo'], label=team)\n",
    "\n",
    "# plt.xlabel('Victoires Cumulées')\n",
    "# plt.ylabel('ELO')\n",
    "# plt.title('ELO en Fonction des Victoires Cumulées')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "# r = np.corrcoef(df_graph['result_cumule'], df_graph['elo'])[0, 1]\n",
    "# print(\"Coefficient de Corrélation de Pearson (r) :\", r)\n",
    "\n",
    "# #sns.pairplot(df_graph[['team', 'opp_assistsat25', 'opp_deathsat25', 'h2h', 'result', 'elo']], hue=\"result\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction of the day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [ ] Récupérer données brutes années précédentes voir laquelle meilleure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming raw data into data with features and without dataleakages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T03:48:27.297236Z",
     "iopub.status.busy": "2025-05-11T03:48:27.296897Z",
     "iopub.status.idle": "2025-05-11T03:51:54.854547Z",
     "shell.execute_reply": "2025-05-11T03:51:54.853335Z",
     "shell.execute_reply.started": "2025-05-11T03:48:27.297210Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_today_train = pd.concat([df_raw_previous_season, df_today_raw_season],axis=0)\n",
    "X, dict_stats = winner_series_create_features(df_today_train, include_objects_columns=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating X_validation and y_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T03:51:54.856925Z",
     "iopub.status.busy": "2025-05-11T03:51:54.856597Z",
     "iopub.status.idle": "2025-05-11T03:51:54.916374Z",
     "shell.execute_reply": "2025-05-11T03:51:54.915084Z",
     "shell.execute_reply.started": "2025-05-11T03:51:54.856899Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#X_validation = X[~X.index.isin(X_saved_train.index.tolist())]\n",
    "#mean_elo = np.percentile([e for sublist in elos for e in sublist], 75)\n",
    "#X = pd.get_dummies(X, columns=[\"split\"])\n",
    "today_teams = df_today_raw_season.teamname.unique()\n",
    "X_train = X[X.date.isin(X_saved_train.date)]\n",
    "y_train = X_train.result\n",
    "X_validation = X[~X.date.isin(X_saved_train.date)]\n",
    "y_validation = X_validation.result\n",
    "X_validation.drop(\"result\", inplace=True, errors=\"ignore\")\n",
    "print(\"X row : \",X.shape[0])\n",
    "print(\"X saved train row : \",X_saved_train.shape[0])\n",
    "print(\"X train row : \",X_train.shape[0])\n",
    "print(\"X validation row : \",X_validation.shape[0])\n",
    "# X row :  33535\n",
    "# X saved train row :  31635\n",
    "# X validation row :  1900"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T03:51:54.918012Z",
     "iopub.status.busy": "2025-05-11T03:51:54.917522Z",
     "iopub.status.idle": "2025-05-11T03:51:54.929254Z",
     "shell.execute_reply": "2025-05-11T03:51:54.928040Z",
     "shell.execute_reply.started": "2025-05-11T03:51:54.917969Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Liste des noms de colonnes (features)\n",
    "enable_manual_bf = False\n",
    "if (enable_manual_bf):\n",
    "    col_fixed = [\"elo_diff\",\"win_rate_diff\",\"h2h_win_rate_AvsB\",\"wardskilled_diff\",\"team kpm_diff\",\"ckpm_diff\",\"controlwardsbought_diff\",\"totalgold_diff\",\"damagetakenperminute_diff\",\"gamelength_diff\",\"wcpm_diff\",\"bo_type\"]\n",
    "    col_fixed = ['goldspent_diff', 'win_rate_last_3_diff', 'wardsplaced_diff', 'triplekills_diff', 'assists_diff', 'monsterkills_diff', 'firstblood_diff', 'elo_diff', 'win_rate_diff', 'h2h_win_rate_AvsB', 'wardskilled_diff', 'team kpm_diff', 'ckpm_diff', 'controlwardsbought_diff', 'totalgold_diff', 'damagetakenperminute_diff', 'gamelength_diff', 'wcpm_diff', 'bo_type']\n",
    "    col_fixed = ['earned gpm_diff', 'win_rate_last_5_diff', 'opp_assistsat15_diff',\n",
    "                 'playoffs', 'csdiffat15_diff', 'firstbaron_diff', \n",
    "                 'goldspent_diff', 'win_rate_last_3_diff', 'wardsplaced_diff', 'triplekills_diff', 'assists_diff', 'monsterkills_diff', 'firstblood_diff', \n",
    "                 'elo_diff', 'win_rate_diff', 'h2h_win_rate_AvsB', 'wardskilled_diff', 'team kpm_diff', 'ckpm_diff', 'controlwardsbought_diff', 'totalgold_diff', \n",
    "                 'damagetakenperminute_diff', 'gamelength_diff', 'wcpm_diff', 'bo_type',\"win_rate_bo_diff\",\"nb_games_diff\",\"win_streak_diff\"]\n",
    "    cols_leakage = [col for col in X_train.columns if (col.endswith('A') or col.endswith('B'))]\n",
    "    feature_names = list(set(X_train.select_dtypes(include=\"number\").columns.tolist()) - set(col_fixed) - set(cols_leakage) - set([\"result\",\"date\"]))\n",
    "    print(feature_names)\n",
    "    best_model_lgb = lgb.LGBMClassifier(n_estimators=82, learning_rate=0.05, n_jobs=4, max_bin=101, verbose=-1)\n",
    "    # Fonction pour entraîner et évaluer un modèle avec une combinaison de features\n",
    "    def evaluate_combination(combination):\n",
    "        #print(f\"Combination: {combination}\",end=' ')\n",
    "        X_train_comb = X_train[combination]\n",
    "        X_test_comb = X_validation[combination]\n",
    "        best_model_lgb.fit(X_train_comb, y_train)\n",
    "        #model_xgb_test.fit(X_train_comb, y_train_LFL, eval_set=[(X_valid_LFL[cols], y_valid_LFL)], verbose=False)\n",
    "        y_pred_lgb = best_model_lgb.predict(X_test_comb)\n",
    "        #y_pred_xgb = model_lgb.predict(X_test_comb)\n",
    "    \n",
    "        acc_lgb = accuracy_score(y_validation, y_pred_lgb)\n",
    "        #acc_xgb = accuracy_score(y_valid_LFL, y_pred_xgb)\n",
    "        return acc_lgb#, acc_xgb\n",
    "    \n",
    "    # Meilleure combinaison et sa performance\n",
    "    best_combination = None\n",
    "    best_performance = 0.67\n",
    "    \n",
    "    # Itération sur toutes les combinaisons de longueurs variables\n",
    "    print(\"Nombre de features à tester :\",len(feature_names))\n",
    "    \n",
    "    for r in range(1, 15):\n",
    "        print(f\"\\nNb features: {r}\")\n",
    "        for combination in itertools.combinations(feature_names, r):\n",
    "            total_combination = list(combination)+col_fixed\n",
    "            performance_lgb = evaluate_combination(total_combination)\n",
    "            #performance_lgb, performance_xgb = evaluate_combination(combination)\n",
    "            if (performance_lgb > best_performance): #| (performance_xgb > best_performance):\n",
    "                best_combination = combination\n",
    "                best_performance = performance_lgb # if performance_xgb < performance_lgb else performance_xgb\n",
    "                print(f\"\\nNew best_perf {best_performance:.4f} with {total_combination}.\", end=' ')\n",
    "    \n",
    "    print(\"Meilleure combinaison de features :\", best_combination)\n",
    "    print(\"Performance du modèle avec cette combinaison :\", best_performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction on today new matchs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T03:51:54.930659Z",
     "iopub.status.busy": "2025-05-11T03:51:54.930301Z",
     "iopub.status.idle": "2025-05-11T03:51:55.481992Z",
     "shell.execute_reply": "2025-05-11T03:51:55.480434Z",
     "shell.execute_reply.started": "2025-05-11T03:51:54.930630Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_lgb = lgb.LGBMClassifier(n_estimators=82, learning_rate=0.05, n_jobs=4, max_bin=101, verbose=-1)\n",
    "a = [\"opp_inhibitors_diff\",\"opp_towers_diff\",\"opp_dragons_diff\",\"opp_barons_diff\",\"deaths_diff\"]\n",
    "col_most_important = ['team kpm_diff', 'gamelength_diff', 'bo_type', 'earnedgold_diff', 'wardskilled_diff', 'elo_diff', 'win_rate_diff',\"opp_inhibitors_diff\"]\n",
    "col_old_best_perf = ['goldspent_diff', 'win_rate_last_3_diff', 'wardsplaced_diff', 'triplekills_diff', 'assists_diff', 'monsterkills_diff', 'firstblood_diff', \n",
    "                 'elo_diff', 'win_rate_diff', 'h2h_win_rate_AvsB', 'wardskilled_diff', 'team kpm_diff', 'ckpm_diff', 'controlwardsbought_diff', 'totalgold_diff', \n",
    "                 'damagetakenperminute_diff', 'gamelength_diff', 'wcpm_diff', 'bo_type',\"win_rate_bo_diff\",\"nb_games_diff\",\"win_streak_diff\"]\n",
    "col_best_perf = ['monsterkillsownjungle_diff', 'opp_xpat15_diff', 'opp_deathsat25_diff', \n",
    "                 'earned gpm_diff', 'win_rate_last_5_diff', 'opp_assistsat15_diff', \n",
    "                 'playoffs', 'csdiffat15_diff', 'firstbaron_diff', 'goldspent_diff', \n",
    "                 'win_rate_last_3_diff', 'wardsplaced_diff', 'triplekills_diff', 'assists_diff', \n",
    "                 'monsterkills_diff', 'firstblood_diff', 'elo_diff', 'win_rate_diff', 'h2h_win_rate_AvsB', \n",
    "                 'wardskilled_diff', 'team kpm_diff', 'ckpm_diff', 'controlwardsbought_diff', 'totalgold_diff', \n",
    "                 'damagetakenperminute_diff', 'gamelength_diff', 'wcpm_diff', 'bo_type', 'win_rate_bo_diff', 'nb_games_diff', 'win_streak_diff']\n",
    "\n",
    "preprocessor_few_col = Pipeline(steps=[\n",
    "    (\"preprocessor\", ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num_col_mean\", SimpleImputer(strategy=\"mean\"), col_most_important)\n",
    "        ],\n",
    "        remainder='drop'\n",
    "    ))\n",
    "])\n",
    "cols_bool = [col for col in X_train.columns if col.startswith(\"split_\")]\n",
    "print(cols_bool)\n",
    "preprocessor_best_perf = Pipeline(steps=[\n",
    "    (\"preprocessor\", ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"keep_cols\", \"passthrough\", col_best_perf),\n",
    "            \n",
    "        ],\n",
    "        remainder='drop'\n",
    "    ))\n",
    "])\n",
    "\n",
    "pipeline_LGB_few_col = Pipeline([\n",
    "    (\"preprocess\", preprocessor_few_col),\n",
    "    (\"model\", model_lgb)\n",
    "])\n",
    "\n",
    "pipeline_LGB_best_perf = Pipeline([\n",
    "    (\"preprocess\", preprocessor_best_perf),\n",
    "    (\"model\", model_lgb)\n",
    "])\n",
    "\n",
    "def incremental_fit_predict(model, X_train, y_train, X_validation, y_validation):\n",
    "    X_train_increment = X_train.copy()\n",
    "    y_train_increment = y_train.copy()\n",
    "    y_pred = pd.Series(dtype=float)\n",
    "    y_proba = pd.Series(dtype=float)\n",
    "    for index, row in X_validation.iterrows():\n",
    "        model.fit(X_train_increment, y_train_increment)\n",
    "\n",
    "        x_input = X_validation.loc[[index]]  # DataFrame à 1 ligne\n",
    "        y_pred_index = model.predict(x_input)[0]\n",
    "        y_proba_index = model.predict_proba(x_input)[0]\n",
    "\n",
    "        y_pred.loc[len(y_pred)] = y_pred_index\n",
    "        y_proba.loc[len(y_proba)] = y_proba_index\n",
    "\n",
    "        # Ajouter la nouvelle ligne et cible au training\n",
    "        X_train_increment = pd.concat([X_train_increment, row.to_frame().T])\n",
    "        y_train_increment = pd.concat([y_train_increment, pd.Series([y_validation.loc[index]])])\n",
    "        if len(y_pred) % 100 == 0:\n",
    "            print(f\"{len(y_pred)} lignes traitées...\")\n",
    "    return y_pred, y_proba\n",
    "        \n",
    "    # Boucle parcourir X_validation\n",
    "        # Modele train on X_train_increment\n",
    "        # Modele predict et proba la ligne en cours dans y_i\n",
    "        # Add y_i a y_predictions total et proba\n",
    "        # Add la ligne en cours a X_train_increment\n",
    "    # return les deux y\n",
    "\n",
    "#y_pred_validation, y_proba_validation = incremental_fit_predict(pipeline_LGB_best_perf,X_saved_train,y_saved_train,X_validation, y_validation )\n",
    "pipeline_LGB_best_perf.fit(X_train, y_train)\n",
    "y_pred_validation = pipeline_LGB_best_perf.predict(X_validation)\n",
    "y_proba_validation = pipeline_LGB_best_perf.predict_proba(X_validation)[:,1]\n",
    "# # display(pipeline_LGB_best_perf.predict_proba(X_validation))\n",
    "# best_model_lgb.fit(X_train[col_best_perf], y_train)\n",
    "# y_pred_validation = best_model_lgb.predict(X_validation[col_best_perf])\n",
    "# y_proba_validation = best_model_lgb.predict_proba(X_validation[col_best_perf])[:,1]\n",
    "# display(best_model_lgb.predict_proba(X_validation[col_best_perf]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving results and calculate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T05:09:30.225024Z",
     "iopub.status.busy": "2025-05-11T05:09:30.223059Z",
     "iopub.status.idle": "2025-05-11T05:09:30.304387Z",
     "shell.execute_reply": "2025-05-11T05:09:30.303345Z",
     "shell.execute_reply.started": "2025-05-11T05:09:30.224925Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "columns_match_logs = [ \"date\", \"true_result\", \"predicted_result\", \"model_confidence_B_win(1)\", \"was_correct\"]\n",
    "\n",
    "log_today = pd.DataFrame(columns=columns_match_logs)\n",
    "log_today[[\"date\"]] = X_validation[[\"date\"]]\n",
    "log_today[\"true_result\"] = y_validation\n",
    "log_today[\"predicted_result\"] = y_pred_validation\n",
    "log_today[\"model_confidence_B_win(1)\"] = y_proba_validation\n",
    "log_today[\"was_correct\"] = y_validation == y_pred_validation\n",
    "\n",
    "match_logs = pd.concat([match_logs, log_today])\n",
    "#match_logs.loc[log_today.index.tolist()] = log_today\n",
    "\n",
    "today_accuracy = log_today.was_correct.mean()\n",
    "global_accuracy = match_logs.was_correct.mean()\n",
    "\n",
    "print(f\"✅ Précision aujourd’hui : {today_accuracy:.2%} {log_today.was_correct.sum()}/{log_today.shape[0]}\")\n",
    "print(f\"📈 Précision globale : {global_accuracy:.2%} {match_logs.was_correct.sum()}/{match_logs.shape[0]}\")\n",
    "display((log_today))\n",
    "high_confidence_B = log_today[log_today.predicted_result == 1][\"model_confidence_B_win(1)\"] >0.8\n",
    "\n",
    "match_logs[\"date_normalize\"] = pd.to_datetime(match_logs[\"date\"]).dt.normalize()\n",
    "success_rate_by_day = match_logs.groupby(\"date_normalize\")[\"was_correct\"].mean().reset_index(name=\"success_rate\")\n",
    "display(success_rate_by_day[success_rate_by_day.success_rate <= 0.5])\n",
    "\n",
    "\n",
    "# X.to_csv(\"X_train.csv\")\n",
    "# match_logs.to_csv(\"match_logs.csv\")\n",
    "# df_today_raw_season.to_csv(\"raw_data_actual_season.csv\")\n",
    "# Ajouter winrate par league, winrate par SPLIT "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert in the database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T03:51:55.538383Z",
     "iopub.status.busy": "2025-05-11T03:51:55.538022Z",
     "iopub.status.idle": "2025-05-11T03:51:56.640248Z",
     "shell.execute_reply": "2025-05-11T03:51:56.639307Z",
     "shell.execute_reply.started": "2025-05-11T03:51:55.538354Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Convert DataFrame rows to list of dicts\n",
    "\n",
    "cols_lol_predict = [\"result\", \"date\",\"teamnameA\",\"teamnameB\"] # Cols needed for the website lol_predict but not for the model\n",
    "df_supabase = X_validation[col_best_perf + cols_lol_predict]\n",
    "#display(X_validation.head())\n",
    "df_supabase = pd.concat([df_supabase, match_logs[[\"predicted_result\",\"model_confidence_B_win(1)\"]]],axis=1)\n",
    "#display(df_supabase.head())\n",
    "df_supabase = df_supabase.reset_index()\n",
    "df_supabase = df_supabase.rename(columns={\n",
    "    \"index\":\"bo_id\",\n",
    "    \"teamnameA\": \"teamname_a\",\n",
    "    \"teamnameB\": \"teamname_b\",\n",
    "    \"predicted_result\":\"prediction\",\n",
    "    \"model_confidence_B_win(1)\":\"prediction_proba\"\n",
    "})\n",
    "# Replace such that all invalid values (NaN, inf) become Python native None (optional)\n",
    "df_supabase = df_supabase.replace([np.nan, np.inf, -np.inf], None)\n",
    "\n",
    "# Now convert to list of dicts while skipping None values per row\n",
    "data_to_insert = [\n",
    "    {k: v for k, v in row.items() if v is not None}\n",
    "    for row in df_supabase.to_dict(orient=\"records\")\n",
    "]\n",
    "\n",
    "#data_to_insert = df_supabase.to_dict(orient=\"records\")\n",
    "\n",
    "\n",
    "# Perform batch insert\n",
    "response = supabase.table(\"matches\").upsert(data_to_insert).execute()\n",
    "print(\"Data upserted with success !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of the failures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T03:51:56.641637Z",
     "iopub.status.busy": "2025-05-11T03:51:56.641323Z",
     "iopub.status.idle": "2025-05-11T03:51:59.418563Z",
     "shell.execute_reply": "2025-05-11T03:51:59.417419Z",
     "shell.execute_reply.started": "2025-05-11T03:51:56.641610Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "wrong_predictions = ~log_today.was_correct # 692 Rows True\n",
    "X_wrong_bc_of_elo =  X_validation[\n",
    "        (wrong_predictions) &\n",
    "        (\n",
    "            (X_validation.elo_diff > 0) & (X_validation.result == 1) |\n",
    "            (X_validation.elo_diff < 0) & (X_validation.result == 0)\n",
    "        )\n",
    "    ]\n",
    "\n",
    "print(f\"Number of wrong predictions : {X_validation[wrong_predictions].shape[0]}\" )\n",
    "print(f\"Number of wrong predictions because of elo : {X_wrong_bc_of_elo.shape[0]} which represents {X_wrong_bc_of_elo.shape[0]*100/X_validation[wrong_predictions].shape[0]:.2f}%\" )\n",
    "print(f\"Mean elo diff above 0 for X_validation :{X_validation[X_validation.elo_diff >0].elo_diff.median()} and for wrong prediction : {X_wrong_bc_of_elo[X_wrong_bc_of_elo.elo_diff >0].elo_diff.median()}\")\n",
    "print(f\"Mean elo diff under 0 for X_validation :{X_validation[X_validation.elo_diff <0].elo_diff.median()} and for wrong prediction : {X_wrong_bc_of_elo[X_wrong_bc_of_elo.elo_diff <0].elo_diff.median()}\")\n",
    "\n",
    "def compare_columns_wrong_predictions(column):\n",
    "    df_compare = pd.concat([X_validation[column].value_counts()*100/X_validation.shape[0], X_wrong_bc_of_elo[column].value_counts()*100/X_wrong_bc_of_elo.shape[0]],axis=1)\n",
    "    df_compare.columns = [f\"% {column} in validation\", f\"% {column} in wrong predictions\"]\n",
    "    df_compare = df_compare.sort_values(f\"% {column} in wrong predictions\", ascending=False).round(2)\n",
    "    df_compare = df_compare[df_compare[f\"% {column} in wrong predictions\"] > (df_compare[f\"% {column} in validation\"] )] # Take only the teams that are much more present in wrong predictions\n",
    "    #display(df_compare)\n",
    "    # Tracer un barplot\n",
    "    # Faire un barplot double\n",
    "    plt.figure(figsize=(14, 15))\n",
    "    \n",
    "    # Barres bleues pour la présence dans le dataset\n",
    "    plt.barh(df_compare.index, df_compare[f\"% {column} in validation\"], color='skyblue', label='Présence dans validation')\n",
    "    \n",
    "    # Barres rouges transparentes par-dessus pour erreurs\n",
    "    plt.barh(df_compare.index, df_compare[f\"% {column} in wrong predictions\"], color='salmon', alpha=0.7, label='Erreurs de prédiction')\n",
    "    \n",
    "    # Ajouter titres et légende\n",
    "    plt.xlabel('Pourcentage (%)', fontsize=12)\n",
    "    plt.title(f'Présence vs Erreurs de prédiction par {column}', fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.gca().invert_yaxis()  # La ligue avec plus d'erreurs en haut\n",
    "    plt.grid(axis='x', linestyle='--', alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "\n",
    "for object_column in [\"league\",\"split\", \"bo_type\", \"nb_games_diff\"]:\n",
    "    compare_columns_wrong_predictions(object_column)\n",
    "\n",
    "# 86\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial load of data TO DELETE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the games with wrong predictions have an elo usually closer. It's thougher games \n",
    "\n",
    "\n",
    "From the graph, we can see that there isn't specially a league or split that is really more difficult to predict. Same for the league seasonnal and international.\n",
    "\n",
    "However, we see that he have much more difficulties when it's a BO3. After adding a feature which is the win_rate for this type of BO we increased the accuracy of 0.41 !\n",
    "\n",
    "Now we'll test the number of game of team implicted, and check others columns which were showing the contrary of elo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T03:51:59.420094Z",
     "iopub.status.busy": "2025-05-11T03:51:59.419678Z",
     "iopub.status.idle": "2025-05-11T03:51:59.668693Z",
     "shell.execute_reply": "2025-05-11T03:51:59.667610Z",
     "shell.execute_reply.started": "2025-05-11T03:51:59.420064Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(f\"Median nb_games_diff above 0 for X_validation :{X_validation[X_validation.nb_games_diff >0].nb_games_diff.median()} and for wrong prediction : {X_wrong_bc_of_elo[X_wrong_bc_of_elo.nb_games_diff >0].nb_games_diff.median()}\")\n",
    "print(f\"Median nb_games_diff under 0 for X_validation :{X_validation[X_validation.nb_games_diff <0].nb_games_diff.median()} and for wrong prediction : {X_wrong_bc_of_elo[X_wrong_bc_of_elo.nb_games_diff <0].nb_games_diff.median()}\")\n",
    "print(f\"Median team new game for X_validation : {X_validation.new_games.sum()} and for wrong prediction : {X_wrong_bc_of_elo.new_games.sum()}\")\n",
    "column = \"teamname\"\n",
    "print(X_validation.select_dtypes(include=\"object\").columns.tolist())\n",
    "df_compareA = pd.concat([X_validation[\"teamnameA\"].value_counts()*100/X_validation.shape[0], X_wrong_bc_of_elo[\"teamnameA\"].value_counts()*100/X_wrong_bc_of_elo.shape[0]],axis=1)\n",
    "df_compareB = pd.concat([X_validation[\"teamnameB\"].value_counts()*100/X_validation.shape[0], X_wrong_bc_of_elo[\"teamnameB\"].value_counts()*100/X_wrong_bc_of_elo.shape[0]],axis=1)\n",
    "\n",
    "df_compareA.columns = [f\"% {column} in validation\", f\"% {column} in wrong predictions\"]\n",
    "df_compareB.columns = [f\"% {column} in validation\", f\"% {column} in wrong predictions\"]\n",
    "df_compare = pd.concat([df_compareA/2, df_compareB/2]) \n",
    "df_compare = df_compare[df_compare[f\"% {column} in wrong predictions\"] > (df_compare[f\"% {column} in validation\"] + 0.2)] # Take only the teams that are much more present in wrong predictions\n",
    "df_compare = df_compare.sort_values(f\"% {column} in wrong predictions\", ascending=False).round(2)\n",
    "#display(df_compare)\n",
    "teams_strongly_wrong = df_compare.index.unique()\n",
    "#print(\"list of teams :\", teams_strongly_wrong)\n",
    "\n",
    "for team in teams_strongly_wrong:\n",
    "    date_min = X_train[(X_train[\"teamnameA\"] == team) | (X_train[\"teamnameB\"] == team)].date.min()\n",
    "    print(f\"1st game of the team {team} : {date_min}\")\n",
    "    \n",
    "\n",
    "\n",
    "#display(df_compare)\n",
    "# Tracer un barplot\n",
    "# Faire un barplot double\n",
    "# plt.figure(figsize=(14, 15))\n",
    "\n",
    "# # Barres bleues pour la présence dans le dataset\n",
    "# plt.barh(df_compare.index, df_compare[f\"% {column} in validation\"], color='skyblue', label='Présence dans validation')\n",
    "\n",
    "# # Barres rouges transparentes par-dessus pour erreurs\n",
    "# plt.barh(df_compare.index, df_compare[f\"% {column} in wrong predictions\"], color='salmon', alpha=0.7, label='Erreurs de prédiction')\n",
    "\n",
    "# # Ajouter titres et légende\n",
    "# plt.xlabel('Pourcentage (%)', fontsize=12)\n",
    "# plt.title(f'Présence vs Erreurs de prédiction par {column}', fontsize=14)\n",
    "# plt.legend()\n",
    "# plt.gca().invert_yaxis()  # La ligue avec plus d'erreurs en haut\n",
    "# plt.grid(axis='x', linestyle='--', alpha=0.5)\n",
    "# plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that half of the team are very recent 2024 or even 2025 and that's why their elo is not accurate.\n",
    "Implementing elo_decay and including features with recent win_rate might helps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T03:51:59.670360Z",
     "iopub.status.busy": "2025-05-11T03:51:59.669993Z",
     "iopub.status.idle": "2025-05-11T03:52:02.486159Z",
     "shell.execute_reply": "2025-05-11T03:52:02.484980Z",
     "shell.execute_reply.started": "2025-05-11T03:51:59.670327Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_cols_contrary_elo = pd.DataFrame(columns=[\"appearance\"])\n",
    "cols_diff = [col for col in X_wrong_bc_of_elo.columns if col.endswith('_diff')]\n",
    "\n",
    "for col in cols_diff:\n",
    "    df_cols_contrary_elo.loc[col] = X_wrong_bc_of_elo[((X_wrong_bc_of_elo[\"elo_diff\"] > 0) & (X_wrong_bc_of_elo[col] < 0)) | ((X_wrong_bc_of_elo[\"elo_diff\"] < 0) & (X_wrong_bc_of_elo[col] > 0))].shape[0]\n",
    "\n",
    "df_cols_contrary_elo = df_cols_contrary_elo.sort_values(\"appearance\", ascending=False)\n",
    "print(f\"Nb wrongs predictions bc of elo {X_wrong_bc_of_elo.shape[0]}\")\n",
    "print(\"cols diff in the other way of the elo\")\n",
    "display(df_cols_contrary_elo)\n",
    "plt.figure(figsize=(14, 15))\n",
    "\n",
    "# Barres bleues pour la présence dans le dataset\n",
    "plt.barh(df_cols_contrary_elo.index, df_cols_contrary_elo[\"appearance\"], color='skyblue', label='Count of appearance')\n",
    "\n",
    "\n",
    "# Ajouter titres et légende\n",
    "plt.xlabel('Number', fontsize=12)\n",
    "plt.title(f'Présence of appearance in contrary of elo', fontsize=14)\n",
    "plt.legend()\n",
    "plt.gca().invert_yaxis()  # La ligue avec plus d'erreurs en haut\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-05-11T03:52:02.487646Z",
     "iopub.status.busy": "2025-05-11T03:52:02.487350Z",
     "iopub.status.idle": "2025-05-11T03:52:02.494001Z",
     "shell.execute_reply": "2025-05-11T03:52:02.492741Z",
     "shell.execute_reply.started": "2025-05-11T03:52:02.487621Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "[\"opp_inhibitors_diff\",\"opp_towers_diff\",\"opp_dragons_diff\",\"opp_barons_diff\",\"deaths_diff\"]\n",
    "# match_logs = pd.DataFrame(columns=columns_match_logs)\n",
    "# match_logs.to_csv(\"match_logs.csv\")\n",
    "# raw_2025_empty = X_2025[0:0]\n",
    "# raw_2025_empty.to_csv(\"raw_data_actual_season.csv\")\n",
    "# X_intuition.to_csv(\"X_train.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Increase accuracy\n",
    "   - Elo decay\n",
    "   - BF hyperparameters\n",
    "   - new features, win_rate_play_offs\n",
    "2. Accuracy by day\n",
    "3. Get next games and predictions for next day\n",
    "   - Finish the function that take in param the name of both teams\n",
    "   - print proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T05:39:07.085373Z",
     "iopub.status.busy": "2025-05-11T05:39:07.084902Z",
     "iopub.status.idle": "2025-05-11T05:39:07.759281Z",
     "shell.execute_reply": "2025-05-11T05:39:07.758049Z",
     "shell.execute_reply.started": "2025-05-11T05:39:07.085342Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Date de demain au format ISO\n",
    "tomorrow = (datetime.utcnow() + timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "print(tomorrow)\n",
    "\n",
    "# API endpoint de Leaguepedia (Liquipedia)\n",
    "url = \"https://lol.fandom.com/api.php\"\n",
    "\n",
    "# Paramètres de la requête Cargo pour interroger les matchs à partir de demain\n",
    "params = {\n",
    "    \"action\": \"cargoquery\",\n",
    "    \"format\": \"json\",\n",
    "    \"tables\": \"MatchSchedule\",\n",
    "    \"fields\": \"MatchSchedule.Team1,MatchSchedule.Team2,MatchSchedule.DateTime_UTC,MatchSchedule.OverviewPage,MatchSchedule.BestOf,MatchSchedule.Round\",\n",
    "    \"where\": f\"MatchSchedule.DateTime_UTC >= '{tomorrow}'\",\n",
    "    \"order_by\": \"MatchSchedule.DateTime_UTC ASC\",\n",
    "    \"limit\": \"500\"  # Augmente si nécessaire\n",
    "}\n",
    "\n",
    "# Requête HTTP\n",
    "response = requests.get(url, params=params)\n",
    "\n",
    "# Vérifier et afficher les résultats\n",
    "if response.status_code == 200:\n",
    "    results = response.json()[\"cargoquery\"]\n",
    "    # Étape 2 : Grouper les matchs par date (UTC, sans l'heure)\n",
    "    match_data = [\n",
    "        {\n",
    "            \"teamnameA\": m[\"title\"][\"Team1\"],\n",
    "            \"teamnameB\": m[\"title\"][\"Team2\"],\n",
    "            \"date\": m[\"title\"][\"DateTime UTC\"],\n",
    "            \"playoffs\": 0 if m[\"title\"][\"Round\"] is None else 1,\n",
    "            \"bo_type\": m[\"title\"][\"BestOf\"],\n",
    "            \"league\": m[\"title\"][\"OverviewPage\"].split(\"/\")[0]\n",
    "        }\n",
    "        for m in results\n",
    "    ]\n",
    "    df = pd.DataFrame(match_data)\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"]).dt.date\n",
    "    \n",
    "    # Étape 3 : Trouver la date de la prochaine journée (la première date du DataFrame)\n",
    "    if not df.empty:\n",
    "        next_match_day = df[\"date\"].min()\n",
    "        df_next_day = df[df[\"date\"] == next_match_day]\n",
    "        #display(df_next_day)\n",
    "    else:\n",
    "        print(\"Aucun match trouvé à partir d'aujourd'hui.\")\n",
    "else:\n",
    "    print(\"Erreur de requête :\", response.status_code)\n",
    "\n",
    "\n",
    "def create_feature_row(row):\n",
    "    for col in col_best_perf:\n",
    "        if (col.endswith(\"_diff\")):\n",
    "            if (col.startswith(\"win_rate_last_\")):\n",
    "                window = int(col.split(\"_\")[3])\n",
    "                row[col] = np.mean(dict_stats.get(row.teamnameA).get(\"result\")[-window:]) - np.mean(dict_stats.get(row.teamnameB).get(\"result\")[-window:])\n",
    "            elif (col == \"win_rate_diff\"):\n",
    "                row[col] = np.mean(dict_stats.get(row.teamnameA).get(\"result\")) - np.mean(dict_stats.get(row.teamnameB).get(\"result\"))\n",
    "            elif (col == \"win_rate_bo_diff\"):\n",
    "                win_rate_bo_A = np.mean(dict_stats[row.teamnameA][f\"bo_{row['bo_type']}\"]) if len(dict_stats[row.teamnameA][f\"bo_{row['bo_type']}\"]) >= 1 else np.mean(dict_stats.get(row.teamnameA).get(\"result\"))\n",
    "                win_rate_bo_B = np.mean(dict_stats[row.teamnameB][f\"bo_{row['bo_type']}\"]) if len(dict_stats[row.teamnameB][f\"bo_{row['bo_type']}\"]) >= 1 else np.mean(dict_stats.get(row.teamnameB).get(\"result\"))\n",
    "                row[col] = win_rate_bo_A - win_rate_bo_B\n",
    "            elif (col == \"nb_games_diff\"):\n",
    "                row[col] = len(dict_stats.get(row.teamnameA).get(\"result\")) - len(dict_stats.get(row.teamnameB).get(\"result\"))\n",
    "            elif (col == \"win_streak_diff\"):\n",
    "                row[col] = dict_stats.get(row.teamnameA).get(\"win_streak\")[-1] - dict_stats.get(row.teamnameB).get(\"win_streak\")[-1]\n",
    "            else:\n",
    "                row[col] = np.mean(dict_stats.get(row.teamnameA).get(col[:-5])) - np.mean(dict_stats.get(row.teamnameB).get(col[:-5]))\n",
    "        elif (col == \"h2h_win_rate_AvsB\"):\n",
    "            if (row.teamnameA < row.teamnameB):\n",
    "                h2h_key = row.teamnameA + row.teamnameB\n",
    "                h2h_win_rate_AvsB = np.mean(dict_stats[h2h_key]) if dict_stats.get(h2h_key) else 0.5\n",
    "            else:\n",
    "                h2h_key = row.teamnameB + row.teamnameA \n",
    "                h2h_win_rate_AvsB = (1 - np.mean(dict_stats[h2h_key])) if dict_stats.get(h2h_key) else 0.5\n",
    "            row[\"h2h_win_rate_AvsB\"] = h2h_win_rate_AvsB\n",
    "    return row\n",
    "\n",
    "display(df_next_day)\n",
    "df_next_day = df_next_day[df_next_day.teamnameA.isin(dict_stats) & (df_next_day.teamnameB.isin(dict_stats)) ]\n",
    "df_next_day = df_next_day.apply(create_feature_row, axis=1).reset_index()\n",
    "display(df_next_day)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions for the next day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T05:39:22.540616Z",
     "iopub.status.busy": "2025-05-11T05:39:22.540193Z",
     "iopub.status.idle": "2025-05-11T05:39:22.577178Z",
     "shell.execute_reply": "2025-05-11T05:39:22.575223Z",
     "shell.execute_reply.started": "2025-05-11T05:39:22.540583Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "y_predict_next_day = pipeline_LGB_best_perf.predict(df_next_day)\n",
    "y_proba_next_day = pipeline_LGB_best_perf.predict_proba(df_next_day)[:,1] # B Win\n",
    "df_next_day[\"prediction\"] = y_predict_next_day\n",
    "df_next_day[\"prediction_proba\"] = y_proba_next_day\n",
    "\n",
    "for i, row in df_next_day.iterrows():\n",
    "    print(f\"h2h: {row.h2h_win_rate_AvsB}, {row.teamnameA} VS {row.teamnameB} : {row.teamnameA if y_predict_next_day[i] == 0 else row.teamnameB} with {y_proba_next_day[i] if y_proba_next_day[i] >= 0.5 else (1-y_proba_next_day[i]):.2%}\")\n",
    "\n",
    "df_next_day.to_csv(\"df_next_day.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding match of the next day in the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-11T06:06:20.892261Z",
     "iopub.status.busy": "2025-05-11T06:06:20.891514Z",
     "iopub.status.idle": "2025-05-11T06:06:21.301638Z",
     "shell.execute_reply": "2025-05-11T06:06:21.300395Z",
     "shell.execute_reply.started": "2025-05-11T06:06:20.892217Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "cols_lol_predict = [\"date\",\"teamnameA\",\"teamnameB\"] # Cols needed for the website lol_predict but not for the model\n",
    "#display(df_supabase_next_day.head())\n",
    "\n",
    "df_supabase_next_day = df_next_day.rename(columns={\n",
    "    \"index\":\"bo_id\",\n",
    "    \"teamnameA\": \"teamname_a\",\n",
    "    \"teamnameB\": \"teamname_b\",\n",
    "})\n",
    "# Replace such that all invalid values (NaN, inf) become Python native None (optional)\n",
    "df_supabase_next_day = df_supabase_next_day.replace([np.nan, np.inf, -np.inf], None)\n",
    "df_supabase_next_day.bo_id = df_supabase_next_day.apply(lambda x : f\"{x.date}-{x.teamname_a}-{x.teamname_b}\", axis=1)\n",
    "df_supabase_next_day.date = df_supabase_next_day.date.astype(\"string\")\n",
    "# Now convert to list of dicts while skipping None values per row\n",
    "data_to_insert_next_day = [\n",
    "    {k: v for k, v in row.items() if v is not None}\n",
    "    for row in df_supabase_next_day.to_dict(orient=\"records\")\n",
    "]\n",
    "#display(df_supabase_next_day)\n",
    "#print(data_to_insert_next_day[0])\n",
    "\n",
    "# Perform batch insert\n",
    "response = supabase.table(\"matches\").upsert(data_to_insert_next_day).execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial test \n",
    "Validated on 2025 : Mean patch better, stratify=y better, game_length_diff useless\n",
    "Google Correlation : \n",
    "### Very low :\n",
    "- xpat25b 0\n",
    "- golddiffat25b 0\n",
    "- csat25B 0\n",
    "- firstdragonA 0\n",
    "- firstbloodA 0\n",
    "- damageshareA 0 (only 2 value ??)\n",
    "- earnedgoldshareA 0 (only 2 value ??)\n",
    "- - totalcs 0 (only 2 value ??)\n",
    "- bo_type 0.001 (feature engineer win rate bo type ?)\n",
    "- playsoffs 0 (feature engineer win rate by playoffs ?)\n",
    "- year -\n",
    "\n",
    "\n",
    "\n",
    "Columns : ['playoffsA', 'patchA', 'gamelengthA', 'killsA', 'deathsA', 'assistsA', 'doublekillsA', 'triplekillsA', 'quadrakillsA', 'pentakillsA', 'firstbloodA', 'team kpmA', 'ckpmA', 'firstdragonA', 'dragonsA', 'opp_dragonsA', 'elementaldrakesA', 'opp_elementaldrakesA', 'infernalsA', 'mountainsA', 'cloudsA', 'oceansA', 'chemtechsA', 'hextechsA', 'eldersA', 'opp_eldersA', 'firstheraldA', 'heraldsA', 'opp_heraldsA', 'void_grubsA', 'opp_void_grubsA', 'firstbaronA', 'baronsA', 'opp_baronsA', 'firsttowerA', 'towersA', 'opp_towersA', 'firstmidtowerA', 'firsttothreetowersA', 'turretplatesA', 'opp_turretplatesA', 'inhibitorsA', 'opp_inhibitorsA', 'damagetochampionsA', 'dpmA', 'damagetakenperminuteA', 'damagemitigatedperminuteA', 'wardsplacedA', 'wpmA', 'wardskilledA', 'wcpmA', 'controlwardsboughtA', 'visionscoreA', 'vspmA', 'totalgoldA', 'earnedgoldA', 'earned gpmA', 'goldspentA', 'gspdA', 'gprA', 'minionkillsA', 'monsterkillsA', 'cspmA', 'goldat10A', 'xpat10A', 'csat10A', 'opp_goldat10A', 'opp_xpat10A', 'opp_csat10A', 'golddiffat10A', 'xpdiffat10A', 'csdiffat10A', 'killsat10A', 'assistsat10A', 'deathsat10A', 'opp_killsat10A', 'opp_assistsat10A', 'opp_deathsat10A', 'goldat15A', 'xpat15A', 'csat15A', 'opp_goldat15A', 'opp_xpat15A', 'opp_csat15A', 'golddiffat15A', 'xpdiffat15A', 'csdiffat15A', 'killsat15A', 'assistsat15A', 'deathsat15A', 'opp_killsat15A', 'opp_assistsat15A', 'opp_deathsat15A', 'from_playoffs-opp_deathsat15_with_B', 'teamnameA', 'teamnameB', 'bo_type', 'result', 'team_A_elo', 'team_B_elo', 'team_A_win_rate', 'team_B_win_rate']\n",
    "## Random Forest\n",
    "19 Rows of EM League : \n",
    "- Cross-validation Accuracy: 0.333\n",
    "- Test Accuracy: 0.25\n",
    "\n",
    "955 rows × 192 of 2025 : \n",
    "- Cross-validation Accuracy: 0.6165\n",
    "- Best Parameters: {'model__max_depth': None, 'model__min_samples_split': 10, 'model__n_estimators': 50}\n",
    "- Test Accuracy: 0.6702\n",
    "\n",
    "# (without unknown team)\n",
    "863 rows × 196 columns of 2025  with nb_games:\n",
    "- Cross-validation Accuracy: 0.6043\n",
    "- Best Parameters: {'model__max_depth': None, 'model__min_samples_split': 2, 'model__n_estimators': 200}\n",
    "- Test Accuracy: 0.6127\n",
    "\n",
    "\n",
    "- Cross-validation Accuracy without nb_games: 0.6145\n",
    "- Best Parameters: {'model__max_depth': None, 'model__min_samples_split': 5, 'model__n_estimators': 50}\n",
    "- Test Accuracy: 0.6301\n",
    "\n",
    "\n",
    "\n",
    "Dropping team name : \n",
    "- Cross-validation Accuracy: 0.6348\n",
    "- Cross-validation Accuracy XGB: 0.6014\n",
    "\n",
    "Increasing to 2024-2025 :\n",
    "- Cross-validation Accuracy Random: 0.6575\n",
    "- Cross-validation Accuracy XGB (128): 0.6582\n",
    "\n",
    "K-multiplier 45 :\n",
    "- Cross-validation Accuracy Random: 0.6613\n",
    "- Cross-validation Accuracy XGB: 0.6593"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6918355,
     "sourceId": 11098361,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
